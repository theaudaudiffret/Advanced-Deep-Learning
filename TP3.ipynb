{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "/PlDLt3cLqbp",
        "id": "W1DrjRmYKDyH"
      },
      "source": [
        "# TP 3  : Graph Neural Networks Architecture\n",
        "\n",
        "**Th√©o Rudkiewicz, Cyriaque Rousselot**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "/PlDLt3cLqbp",
        "id": "q74c95hUKDyI"
      },
      "source": [
        "# TUTORIAL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "/PlDLt3cLqbp",
        "id": "9jOQdIdzKDyJ"
      },
      "source": [
        "### Install Pytorch Geometric\n",
        "\n",
        "To handle graph data, we use the library Pytorch Geometric : https://pytorch-geometric.readthedocs.io/en/latest/\n",
        "\n",
        "*   If you use _Google Colab_, simply run the following cell to install Pytorch Geometric (**advised**).\n",
        "*   If you plan using your _own environment_, follow the documentation to install Pytorch Geometric : https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html and skip the following cell."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 81,
=======
      "execution_count": 1,
>>>>>>> dfb859624f482885dfbb3393c14043067cdd492b
      "metadata": {
        "cell_ktag": "/PlDLt3cLqbp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhkGmYGXKDyJ",
<<<<<<< HEAD
        "outputId": "9afe5e4f-0ba7-43c0-c937-d181608d81d1"
=======
        "outputId": "746f0507-c283-40de-b153-5417bda6ff31"
>>>>>>> dfb859624f482885dfbb3393c14043067cdd492b
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.5.1+cu124.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.11/dist-packages (2.1.2+pt25cu124)\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.5.1+cu124.html\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.11/dist-packages (0.6.18+pt25cu124)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse) (1.26.4)\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.5.1+cu124.html\n",
            "Requirement already satisfied: torch-cluster in /usr/local/lib/python3.11/dist-packages (1.6.3+pt25cu124)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-cluster) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-cluster) (1.26.4)\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.5.1+cu124.html\n",
            "Requirement already satisfied: torch-spline-conv in /usr/local/lib/python3.11/dist-packages (1.2.2+pt25cu124)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.13)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2024.10.0)\n",
<<<<<<< HEAD
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.5)\n",
=======
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
>>>>>>> dfb859624f482885dfbb3393c14043067cdd492b
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
<<<<<<< HEAD
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.4.6)\n",
=======
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.5.0)\n",
>>>>>>> dfb859624f482885dfbb3393c14043067cdd492b
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "########## INSTALL TORCH GEOMETRIC ##################\n",
        "# https://pytorch-geometric.readthedocs.io/en/latest/\n",
        "#####################################################\n",
        "import torch\n",
        "\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "    return version.split(\"+\")[0]\n",
        "\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "\n",
        "\n",
        "def format_cuda_version(version):\n",
        "    return \"cu\" + version.replace(\".\", \"\")\n",
        "\n",
        "\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "\n",
        "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "/PlDLt3cLqbp",
        "id": "B1f-SaodKDyK"
      },
      "source": [
        "### Import required packages\n",
        "\n",
        "Run the following cell to import all required packages. This cell **must not** be modified.\n",
        "\n",
        "To significantly accelerate your training, it is advised to use GPU. Using Google Colab, you need to activate it :\n",
        "\n",
        "*   Edit --> Notebook Setting --> Hardware accelerator --> GPU"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 82,
      "metadata": {
        "cell_ktag": "/PlDLt3cLqbp",
        "id": "9OsinppxKDyK"
      },
      "outputs": [],
=======
      "execution_count": 7,
      "metadata": {
        "cell_ktag": "/PlDLt3cLqbp",
        "id": "9OsinppxKDyK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "1256c4b6-d855-402c-d6d5-33061cdf8354"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "partially initialized module 'torch_geometric' has no attribute 'typing' (most likely due to a circular import)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-dea47d7ab212>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgraphnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPPI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhome\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_home_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_home_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_mps_available\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_xpu_available\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0misinstance\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_torch_instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_debug_enabled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_debug\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/isinstance.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtyping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWITH_PT20\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'torch_geometric' has no attribute 'typing' (most likely due to a circular import)"
          ]
        }
      ],
>>>>>>> dfb859624f482885dfbb3393c14043067cdd492b
      "source": [
        "#####################################################\n",
        "################## PACKAGES #########################\n",
        "#####################################################\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch_geometric.nn as graphnn\n",
        "from sklearn.metrics import f1_score\n",
        "from torch_geometric.datasets import PPI\n",
        "from torch_geometric.loader import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "/PlDLt3cLqbp",
        "id": "i63agkMnKDyK"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "We use the Protein-Protein Interaction (PPI) network dataset which includes:\n",
        "- 20 graphs for training\n",
        "- 2 graphs for validation\n",
        "- 2 graphs for testing\n",
        "\n",
        "One graph of the PPI dataset has on average 2372 nodes. Each node has:\n",
        "- 50 features : positional gene sets / motif gene / immunological signatures ...\n",
        "- 121 (binary) labels : gene ontology sets (way to classify gene products like proteins).\n",
        "\n",
        "**This problem aims to predict, for a given PPI graph, the correct nodes' labels**.\n",
        "\n",
        "**It is a node (multi-label) classification task** (trained using supervised learning, with labels to be predicted for each node).\n",
        "\n",
        "For your curiosity, more detailed information on the dataset and some applications:\n",
        "- https://cs.stanford.edu/~jure/pubs/pathways-psb18.pdf\n",
        "- https://arxiv.org/abs/1707.04638\n",
        "\n",
        "To understand how a graph data is implemented in Pytorch Geometric, refer to : https://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html\n"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 83,
      "metadata": {
        "cell_ktag": "/PlDLt3cLqbp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "814oU_BrKDyK",
        "outputId": "b773df7c-a546-4fda-dd4c-c6cab974160e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://data.dgl.ai/dataset/ppi.zip\n",
            "Extracting ./ppi.zip\n",
            "Processing...\n",
            "/usr/local/lib/python3.11/dist-packages/networkx/readwrite/json_graph/node_link.py:287: FutureWarning: \n",
            "The default value will be changed to `edges=\"edges\" in NetworkX 3.6.\n",
            "\n",
            "To make this warning go away, explicitly set the edges kwarg, e.g.:\n",
            "\n",
            "  nx.node_link_graph(data, edges=\"links\") to preserve current behavior, or\n",
            "  nx.node_link_graph(data, edges=\"edges\") for forward compatibility.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples in the train dataset:  20\n",
            "Number of samples in the val dataset:  2\n",
            "Number of samples in the test dataset:  2\n",
            "Output of one sample from the train dataset:  Data(x=[1767, 50], edge_index=[2, 32318], y=[1767, 121])\n",
            "Edge_index :\n",
            "tensor([[   0,    0,    0,  ..., 1744, 1745, 1749],\n",
            "        [ 372, 1101,  766,  ..., 1745, 1744, 1739]])\n",
            "Number of features per node:  50\n",
            "Number of classes per node:  121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        }
      ],
=======
      "execution_count": null,
      "metadata": {
        "cell_ktag": "/PlDLt3cLqbp",
        "id": "814oU_BrKDyK"
      },
      "outputs": [],
>>>>>>> dfb859624f482885dfbb3393c14043067cdd492b
      "source": [
        "### LOAD DATASETS\n",
        "\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "# Train Dataset\n",
        "train_dataset = PPI(root=\"\", split=\"train\")\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
        "# Val Dataset\n",
        "val_dataset = PPI(root=\"\", split=\"val\")\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "# Test Dataset\n",
        "test_dataset = PPI(root=\"\", split=\"test\")\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Number of features and classes\n",
        "n_features, n_classes = train_dataset[0].x.shape[1], train_dataset[0].y.shape[1]\n",
        "\n",
        "print(\"Number of samples in the train dataset: \", len(train_dataset))\n",
        "print(\"Number of samples in the val dataset: \", len(test_dataset))\n",
        "print(\"Number of samples in the test dataset: \", len(test_dataset))\n",
        "print(\"Output of one sample from the train dataset: \", train_dataset[0])\n",
        "print(\"Edge_index :\")\n",
        "print(train_dataset[0].edge_index)\n",
        "print(\"Number of features per node: \", n_features)\n",
        "print(\"Number of classes per node: \", n_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "/PlDLt3cLqbp",
        "id": "xDBiC6kyKDyK"
      },
      "source": [
        "### Define a basic Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "/PlDLt3cLqbp",
        "id": "q849i_CBKDyK"
      },
      "source": [
        "Here we define a very simple Graph Neural Network model which will be used as our baseline. This model consists of three graph convolutional layers (from https://arxiv.org/pdf/1609.02907.pdf). The first two layers computes 256 features, followed by an ELU activation function. The last layer is used for (multi-label) classification task, computing 121 features (for each node)."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 84,
=======
      "execution_count": null,
>>>>>>> dfb859624f482885dfbb3393c14043067cdd492b
      "metadata": {
        "cell_ktag": "/PlDLt3cLqbp",
        "id": "-y3egROjKDyL"
      },
      "outputs": [],
      "source": [
        "#####################################################\n",
        "################## MODEL ############################\n",
        "#####################################################\n",
        "class BasicGraphModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.graphconv1 = graphnn.GCNConv(input_size, hidden_size)\n",
        "        self.graphconv2 = graphnn.GCNConv(hidden_size, hidden_size)\n",
        "        self.graphconv3 = graphnn.GCNConv(hidden_size, output_size)\n",
        "\n",
        "        self.elu = nn.ELU()\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.graphconv1(x, edge_index)\n",
        "        x = self.elu(x)\n",
        "        x = self.graphconv2(x, edge_index)\n",
        "        x = self.elu(x)\n",
        "        x = self.graphconv3(x, edge_index)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATConv\n",
        "\n",
        "class BasicGraphModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, heads=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.gatconv1 = GATConv(input_size, hidden_size, heads=heads, dropout=dropout)\n",
        "        self.gatconv2 = GATConv(hidden_size * heads, hidden_size, heads=heads, dropout=dropout)\n",
        "        self.gatconv3 = GATConv(hidden_size * heads, output_size, heads=6, concat=False, dropout=dropout)\n",
        "\n",
        "        self.elu = nn.ELU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Projection layers for skip connections (to match dimensions)\n",
        "        self.proj1 = nn.Linear(input_size, hidden_size * heads) if input_size != hidden_size * heads else nn.Identity()\n",
        "        self.proj2 = nn.Linear(hidden_size * heads, hidden_size * heads) if hidden_size * heads != hidden_size * heads else nn.Identity()\n",
        "        self.proj3 = nn.Linear(hidden_size * heads, output_size) if hidden_size * heads != output_size else nn.Identity()\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # First GAT layer with skip connection\n",
        "        identity1 = self.proj1(x)\n",
        "        x = self.gatconv1(x, edge_index)\n",
        "        x = self.elu(x) + identity1\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Second GAT layer with skip connection\n",
        "        identity2 = self.proj2(x)\n",
        "        x = self.gatconv2(x, edge_index)\n",
        "        x = self.elu(x) + identity2\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Third GAT layer with skip connection, plus skip from input\n",
        "        identity3 = self.proj3(x)\n",
        "        x = self.gatconv3(x, edge_index)\n",
        "        x = x + identity3\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "fP7p-NMLWk7K"
      },
<<<<<<< HEAD
      "execution_count": 85,
=======
      "execution_count": null,
>>>>>>> dfb859624f482885dfbb3393c14043067cdd492b
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "/PlDLt3cLqbp",
        "id": "cdjqawrYKDyL"
      },
      "source": [
        "Next function is designed to evaluate the performance of the model, computing the F1-Score"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 86,
=======
      "execution_count": null,
>>>>>>> dfb859624f482885dfbb3393c14043067cdd492b
      "metadata": {
        "cell_ktag": "/PlDLt3cLqbp",
        "id": "eHvzoXQkKDyL"
      },
      "outputs": [],
      "source": [
        "#####################################################\n",
        "############### TEST FUNCTION #######################\n",
        "#####################################################\n",
        "def evaluate(model, loss_fcn, device, dataloader):\n",
        "    score_list_batch = []\n",
        "\n",
        "    model.eval()\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        batch = batch.to(device)\n",
        "        output = model(batch.x, batch.edge_index)\n",
        "        loss_test = loss_fcn(output, batch.y)\n",
        "        predict = np.where(output.detach().cpu().numpy() >= 0, 1, 0)\n",
        "        score = f1_score(batch.y.cpu().numpy(), predict, average=\"micro\")\n",
        "        score_list_batch.append(score)\n",
        "\n",
        "    return np.array(score_list_batch).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "/PlDLt3cLqbp",
        "id": "HQEnMNWpKDyL"
      },
      "source": [
        "Next we construct the function to train the model."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 87,
=======
      "execution_count": null,
>>>>>>> dfb859624f482885dfbb3393c14043067cdd492b
      "metadata": {
        "cell_ktag": "/PlDLt3cLqbp",
        "id": "qFHivva9KDyL"
      },
      "outputs": [],
      "source": [
        "#####################################################\n",
        "############## TRAIN FUNCTION #######################\n",
        "#####################################################\n",
        "def train(model, loss_fcn, device, optimizer, max_epochs, train_dataloader, val_dataloader):\n",
        "    epoch_list = []\n",
        "    scores_list = []\n",
        "\n",
        "    # loop over epochs\n",
        "    for epoch in range(max_epochs):\n",
        "        model.train()\n",
        "        losses = []\n",
        "        # loop over batches\n",
        "        for i, train_batch in enumerate(train_dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            train_batch_device = train_batch.to(device)\n",
        "            # logits is the output of the model\n",
        "            logits = model(train_batch_device.x, train_batch_device.edge_index)\n",
        "            # compute the loss\n",
        "            loss = loss_fcn(logits, train_batch_device.y)\n",
        "            # optimizer step\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            losses.append(loss.item())\n",
        "        loss_data = np.array(losses).mean()\n",
        "        print(\"Epoch {:05d} | Loss: {:.4f}\".format(epoch + 1, loss_data))\n",
        "\n",
        "        if epoch % 5 == 0:\n",
        "            # evaluate the model on the validation set\n",
        "            # computes the f1-score (see next function)\n",
        "            score = evaluate(model, loss_fcn, device, val_dataloader)\n",
        "            print(\"F1-Score: {:.4f}\".format(score))\n",
        "            scores_list.append(score)\n",
        "            epoch_list.append(epoch)\n",
        "\n",
        "    return epoch_list, scores_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "/PlDLt3cLqbp",
        "id": "nY8OzA-AKDyL"
      },
      "source": [
        "Let's train this model !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_ktag": "/PlDLt3cLqbp",
<<<<<<< HEAD
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "likUw7RkKDyL",
        "outputId": "c0019f3e-8f29-4b2c-b487-1eba63e72a6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Device:  cuda\n",
            "Epoch 00001 | Loss: 1.1170\n",
            "F1-Score: 0.4549\n",
            "Epoch 00002 | Loss: 0.6473\n",
            "Epoch 00003 | Loss: 0.5498\n",
            "Epoch 00004 | Loss: 0.5113\n",
            "Epoch 00005 | Loss: 0.4898\n",
            "Epoch 00006 | Loss: 0.4728\n",
            "F1-Score: 0.5934\n",
            "Epoch 00007 | Loss: 0.4580\n",
            "Epoch 00008 | Loss: 0.4460\n",
            "Epoch 00009 | Loss: 0.4341\n",
            "Epoch 00010 | Loss: 0.4220\n"
          ]
        }
      ],
=======
        "id": "likUw7RkKDyL"
      },
      "outputs": [],
>>>>>>> dfb859624f482885dfbb3393c14043067cdd492b
      "source": [
        "### DEVICE GPU OR CPU : will select GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"\\nDevice: \", device)\n",
        "\n",
        "### Max number of epochs\n",
        "max_epochs = 10\n",
        "\n",
        "### DEFINE THE MODEL\n",
        "basic_model = BasicGraphModel(input_size=n_features, hidden_size=256, output_size=n_classes, heads=6).to(\n",
        "    device\n",
        ")\n",
        "\n",
        "### DEFINE LOSS FUNCTION\n",
        "loss_fcn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "### DEFINE OPTIMIZER\n",
        "optimizer = torch.optim.Adam(basic_model.parameters(), lr=0.005)\n",
        "\n",
        "### TRAIN THE MODEL\n",
        "epoch_list, basic_model_scores = train(\n",
        "    basic_model,\n",
        "    loss_fcn,\n",
        "    device,\n",
        "    optimizer,\n",
        "    max_epochs,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "/PlDLt3cLqbp",
        "id": "8ZIZBhwdKDyL"
      },
      "source": [
        "Let's evaluate the performance of this basic model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_ktag": "/PlDLt3cLqbp",
<<<<<<< HEAD
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "14iA0UhVKDyM",
        "outputId": "f30a7802-970c-4bc4-8269-0faf83d4be0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Basic Model : F1-Score on the validation set: 0.6670\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
=======
        "id": "14iA0UhVKDyM"
      },
      "outputs": [],
>>>>>>> dfb859624f482885dfbb3393c14043067cdd492b
      "source": [
        "### F1-SCORE ON VALID DATASET\n",
        "score_valid = evaluate(basic_model, loss_fcn, device, val_dataloader)\n",
        "print(\"Basic Model : F1-Score on the validation set: {:.4f}\".format(score_valid))\n",
        "\n",
        "\n",
        "### PLOT EVOLUTION OF F1-SCORE W.R.T EPOCHS\n",
        "def plot_f1_score(epoch_list, scores):\n",
        "    plt.figure(figsize=[10, 5])\n",
        "    plt.plot(epoch_list, scores)\n",
        "    plt.title(\"Evolution of F1-Score w.r.t epochs\")\n",
        "    plt.ylim([0.0, 1.0])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_f1_score(epoch_list, basic_model_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "/PlDLt3cLqbp",
        "id": "1JAoWUvrKDyM"
      },
      "source": [
        "# QUESTIONS\n",
        "\n",
        "## Grading\n",
        "\n",
        "You will be graded on 5 questions. You will need to provide at least 4 files :\n",
        "1. This Notebook\n",
        "2. `class_model_gnn.py`\n",
        "3. `model.pth` (the file **must be of size less than 50Mo** but 20Mo should be enough to get a very good model)\n",
        "4. `conv_as_message_passing.py`\n",
        "\n",
        "If the function you defined passes all the tests, you will get the full grade. Otherwise we  will look at the intermediate questions in the notebook to give you partial credit.\n",
        "\n",
        "\n",
        "\n",
        " Please provide clear, short and __bold font__ answers.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "/PlDLt3cLqbp",
        "id": "lBDJH-5rKDyM"
      },
      "source": [
        "> Question 1 : Design, build and train a model with a F1-score higher than 93% on validation set (**HINT :** https://arxiv.org/pdf/1710.10903.pdf).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "/PlDLt3cLqbp",
        "id": "K6b21wqVKDyM"
      },
      "source": [
        " Provide two files : (https://pytorch.org/tutorials/beginner/saving_loading_models.html)\n",
        " -  a file  `class_model_gnn.py` containing the class inheriting from `torch.nn.Module` architecture of your final model to load\n",
        " -  a `model.pth` file : the model weights\n",
        "\n",
        " We will  test your model on final F1-Score on a test set. You must not use the test set for hyperparameter training.\n",
        "\n",
        "Intermediate question :\n",
        "\n",
        " Provide the script for training, and a plot of the training loss.  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATConv\n",
        "\n",
        "class StudentModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, heads_1=8, heads_2=10, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.gatconv1 = GATConv(input_size, hidden_size, heads=heads_1, dropout=dropout)\n",
        "        self.gatconv2 = GATConv(hidden_size * heads_1, hidden_size, heads=heads_1, dropout=dropout)\n",
        "        self.gatconv3 = GATConv(hidden_size * heads_1, output_size, heads=heads_2, concat=False, dropout=dropout)\n",
        "\n",
        "        self.elu = nn.ELU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Projection layers for skip connections (to match dimensions)\n",
        "        self.proj1 = nn.Linear(input_size, hidden_size * heads_1)\n",
        "        self.proj2 = nn.Identity()\n",
        "        self.proj3 = nn.Linear(hidden_size * heads_1, output_size)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # First GAT layer with skip connection\n",
        "        identity1 = self.proj1(x)\n",
        "        x = self.gatconv1(x, edge_index)\n",
        "        x = self.elu(x) + identity1\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Second GAT layer with skip connection\n",
        "        identity2 = self.proj2(x)\n",
        "        x = self.gatconv2(x, edge_index)\n",
        "        x = self.elu(x) + identity2\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Third GAT layer with skip connection, plus skip from input\n",
        "        identity3 = self.proj3(x)\n",
        "        x = self.gatconv3(x, edge_index)\n",
        "        x = x + identity3\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, GATConv):\n",
        "                for param in m.parameters():\n",
        "                    if param.dim() > 1:  # Only apply Xavier to weight matrices\n",
<<<<<<< HEAD
        "                        nn.init.xavier_normal_(param)"
      ],
      "metadata": {
        "id": "IT47noWDtcjE"
      },
      "execution_count": 94,
=======
        "                        nn.init.xavier_normal_(param)\n"
      ],
      "metadata": {
        "id": "T1e67iF28xph"
      },
      "execution_count": null,
>>>>>>> dfb859624f482885dfbb3393c14043067cdd492b
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_student(model, loss_fcn, device, optimizer, max_epochs, train_dataloader, val_dataloader):\n",
        "    epoch_list = []\n",
        "    scores_list = []\n",
        "    all_losses = []\n",
        "    # loop over epochs\n",
        "    for epoch in range(max_epochs):\n",
        "        model.train()\n",
        "        losses = []\n",
        "        # loop over batches\n",
        "        for i, train_batch in enumerate(train_dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            train_batch_device = train_batch.to(device)\n",
        "            # logits is the output of the model\n",
        "            logits = model(train_batch_device.x, train_batch_device.edge_index)\n",
        "            # compute the loss\n",
        "            loss = loss_fcn(logits, train_batch_device.y)\n",
        "            # optimizer step\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            all_losses.append(loss.item())\n",
        "        loss_data = np.array(losses).mean()\n",
        "        print(\"Epoch {:05d} | Loss: {:.4f}\".format(epoch + 1, loss_data))\n",
        "\n",
        "        if epoch % 5 == 0:\n",
        "            # evaluate the model on the validation set\n",
        "            # computes the f1-score (see next function)\n",
        "            score = evaluate(model, loss_fcn, device, val_dataloader)\n",
        "            print(\"F1-Score: {:.4f}\".format(score))\n",
        "            scores_list.append(score)\n",
        "            epoch_list.append(epoch)\n",
        "        losses.append(loss_data)\n",
        "\n",
        "    return epoch_list, scores_list, all_losses"
      ],
      "metadata": {
        "id": "EAeERX7q8fj-"
      },
<<<<<<< HEAD
      "execution_count": 97,
=======
      "execution_count": null,
>>>>>>> dfb859624f482885dfbb3393c14043067cdd492b
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### DEVICE GPU OR CPU : will select GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"\\nDevice: \", device)\n",
        "\n",
        "### Max number of epochs\n",
        "max_epochs = 160\n",
        "\n",
        "### DEFINE THE MODEL\n",
        "model = StudentModel(input_size=n_features, hidden_size=256, output_size=n_classes, heads_1=8, heads_2=12).to(\n",
        "    device\n",
        ")\n",
        "\n",
        "### DEFINE LOSS FUNCTION\n",
        "loss_fcn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "### DEFINE OPTIMIZER\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0025)\n",
        "\n",
        "### TRAIN THE MODEL\n",
        "epoch_list, model_scores, losses = train_student(\n",
        "    model,\n",
        "    loss_fcn,\n",
        "    device,\n",
        "    optimizer,\n",
        "    max_epochs,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        ")"
      ],
      "metadata": {
<<<<<<< HEAD
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "FGQJUAIf8ndO",
        "outputId": "3b42d3a3-a694-470a-ce21-8520748879b3"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Device:  cuda\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 932.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 796.12 MiB is free. Process 14212 has 13.96 GiB memory in use. Of the allocated memory 11.10 GiB is allocated by PyTorch, and 2.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-99-2289b34ab960>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m### TRAIN THE MODEL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m epoch_list, model_scores, losses = train_student(\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mloss_fcn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-97-364d6729cbf4>\u001b[0m in \u001b[0;36mtrain_student\u001b[0;34m(model, loss_fcn, device, optimizer, max_epochs, train_dataloader, val_dataloader)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fcn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batch_device\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m# optimizer step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m#all_losses.append(loss.item())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 932.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 796.12 MiB is free. Process 14212 has 13.96 GiB memory in use. Of the allocated memory 11.10 GiB is allocated by PyTorch, and 2.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
=======
        "id": "FGQJUAIf8ndO"
      },
      "execution_count": null,
      "outputs": []
>>>>>>> dfb859624f482885dfbb3393c14043067cdd492b
    },
    {
      "cell_type": "code",
      "source": [
        "### F1-SCORE ON VALID DATASET\n",
        "score_valid = evaluate(model, loss_fcn, device, val_dataloader)\n",
        "print(\"Basic Model : F1-Score on the validation set: {:.4f}\".format(score_valid))\n",
        "\n",
        "\n",
        "### PLOT EVOLUTION OF F1-SCORE W.R.T EPOCHS\n",
        "def plot_f1_score(epoch_list, scores):\n",
        "    plt.figure(figsize=[10, 5])\n",
        "    plt.plot(epoch_list, scores)\n",
        "    plt.title(\"Evolution of F1-Score w.r.t epochs\")\n",
        "    plt.ylim([0.0, 1.0])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_f1_score(epoch_list, model_scores)"
      ],
      "metadata": {
        "id": "tj7NUJn-ZC38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 90,
=======
      "execution_count": null,
>>>>>>> dfb859624f482885dfbb3393c14043067cdd492b
      "metadata": {
        "cell_ktag": "/PlDLt3cLqbp",
        "id": "zrTacHJTKDyM"
      },
      "outputs": [],
      "source": [
<<<<<<< HEAD
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from class_model_gnn import StudentModel\n",
        "\n",
        "# Initialize model\n",
        "model = StudentModel()\n",
=======
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# # Initialize model\n",
        "# model = StudentModel()\n",
>>>>>>> dfb859624f482885dfbb3393c14043067cdd492b
        "\n",
        "# ## Save the model\n",
        "# torch.save(model.state_dict(), \"model.pth\")\n",
        "\n",
        "\n",
        "# ### This is the part we will run in the inference to grade your model\n",
        "# ## Load the model\n",
        "# model = StudentModel()  # !  Important : No argument\n",
        "# model.load_state_dict(torch.load(\"model.pth\", weights_only=True))\n",
        "# model.eval()\n",
        "# print(\"Model loaded successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(1600), losses)\n"
      ],
      "metadata": {
<<<<<<< HEAD
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "AmYVqwCp8HZf",
        "outputId": "8207a040-4253-4123-a764-9c3baf07b288"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f4691add3d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGdCAYAAAAxCSikAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAARHZJREFUeJzt3XlcVOX+B/DPsA37sG+CLO4rGCqSlnmjkLymLWb+TM2bdrPlZpQat9LWiy3XtDJp0dRbud3MbmqokUsWSoCIqCEoCAjDpjAsss7z+4M8OoKOY8Achs/79TqvO3POc575Pl2d+XjOc85RCCEEiIiIiGTMzNgFEBEREenDwEJERESyx8BCREREssfAQkRERLLHwEJERESyx8BCREREssfAQkRERLLHwEJERESyZ2HsAtqDVqtFYWEhHBwcoFAojF0OERER3QAhBKqqquDj4wMzs+sfQzGJwFJYWAg/Pz9jl0FEREQ3IT8/H76+vtdtYxKBxcHBAUDLgB0dHY1cDREREd0IjUYDPz8/6Xf8ekwisFw6DeTo6MjAQkRE1MXcyHQOTrolIiIi2WNgISIiItljYCEiIiLZY2AhIiIi2WNgISIiItljYCEiIiLZY2AhIiIi2WNgISIiItljYCEiIiLZY2AhIiIi2WNgISIiItljYCEiIiLZY2DRo/JiI+L2n8a5iovGLoWIiKjbYmDR46Vvj2HpD79j8spfjF0KERFRt8XAosfB7DIAQGlVvZErISIi6r4YWIiIiEj2GFiIiIhI9hhYiIiISPYYWIiIiEj2GFiIiIhI9hhYiIiISPYYWPQQwtgVEBEREQMLERERyR4Dix4KhbErICIiIgYWPXhKiIiIyPgYWIiIiEj2GFiIiIhI9hhYiIiISPYYWIiIiEj2GFiIiIhI9hhYiIiISPYYWIiIiEj2GFiIiIhI9gwOLAcOHMDEiRPh4+MDhUKBbdu2Xbf9o48+CoVC0WoZNGiQ1ObVV19ttb1///4GD4aIiIhMk8GBpaamBsHBwVi5cuUNtV+xYgWKioqkJT8/Hy4uLpgyZYpOu0GDBum0O3jwoKGlERERkYmyMHSHqKgoREVF3XB7lUoFlUolvd+2bRsuXLiA2bNn6xZiYQEvLy9DyyEiIqJuoNPnsKxevRoRERHw9/fXWZ+VlQUfHx8EBQVh+vTpyMvLu2Yf9fX10Gg0OgsRERGZrk4NLIWFhfjhhx8wZ84cnfVhYWFYu3Yt4uPjsWrVKuTk5OC2225DVVVVm/3ExsZKR25UKhX8/Pw6o3wiIiIykk4NLOvWrYOTkxMmT56ssz4qKgpTpkzB0KFDERkZiZ07d6KiogKbN29us5+YmBhUVlZKS35+fidUT0RERMZi8ByWmyWEwJo1azBjxgxYWVldt62TkxP69u2L7OzsNrcrlUoolcqOKJOIiIhkqNOOsOzfvx/Z2dl47LHH9Latrq7G6dOn4e3t3QmVERERkdwZHFiqq6uRlpaGtLQ0AEBOTg7S0tKkSbIxMTGYOXNmq/1Wr16NsLAwDB48uNW2F154Afv370dubi5+/fVX3HfffTA3N8e0adMMLY+IiIhMkMGnhJKTkzFu3DjpfXR0NABg1qxZWLt2LYqKilpd4VNZWYlvvvkGK1asaLPPgoICTJs2DeXl5XB3d8eYMWNw6NAhuLu7G1oeERERmSCDA8sdd9wBIcQ1t69du7bVOpVKhdra2mvus3HjRkPL6DTXGysRERF1Dj5LiIiIiGSPgYWIiIhkj4GFiIiIZI+BhYiIiGSPgYWIiIhkj4GFiIiIZI+BhYiIiGSPgYWIiIhkj4GFiIiIZI+BhYiIiGSPgYWIiIhkj4GFiIiIZI+BhYiIiGSPgYWIiIhkj4GFiIiIZI+BhYiIiGSPgYWIiIhkj4GFiIiIZI+BhYiIiGSPgUUPYewCiIiIiIGFiIiI5I+BRQ+FsQsgIiIiBhZ9eEqIiIjI+BhYiIiISPYYWIiIiEj2GFiIiIhI9hhYiIiISPYYWIiIiEj2GFiIiIhI9hhYiIiISPYYWIiIiEj2GFiIiIhI9hhYiIiISPYYWIiIiEj2DA4sBw4cwMSJE+Hj4wOFQoFt27Zdt/2+ffugUChaLWq1WqfdypUrERAQAGtra4SFhSEpKcnQ0oiIiMhEGRxYampqEBwcjJUrVxq0X2ZmJoqKiqTFw8ND2rZp0yZER0djyZIlSE1NRXBwMCIjI1FSUmJoeURERGSCLAzdISoqClFRUQZ/kIeHB5ycnNrctmzZMsydOxezZ88GAMTFxWHHjh1Ys2YNXnzxRYM/i4iIiExLp81hCQkJgbe3N+666y788ssv0vqGhgakpKQgIiLiclFmZoiIiEBiYmJnlUdEREQy1uGBxdvbG3Fxcfjmm2/wzTffwM/PD3fccQdSU1MBAGVlZWhuboanp6fOfp6enq3muVxSX18PjUajs3QY0XFdExER0Y0x+JSQofr164d+/fpJ72+99VacPn0a77//Pv7zn//cVJ+xsbF47bXX2qtEIiIikjmjXNY8cuRIZGdnAwDc3Nxgbm6O4uJinTbFxcXw8vJqc/+YmBhUVlZKS35+fscVq+i4romIiOjGGCWwpKWlwdvbGwBgZWWF0NBQJCQkSNu1Wi0SEhIQHh7e5v5KpRKOjo46S4fhKSEiIiKjM/iUUHV1tXR0BABycnKQlpYGFxcX9OzZEzExMTh37hzWr18PAFi+fDkCAwMxaNAg1NXV4fPPP8dPP/2E3bt3S31ER0dj1qxZGD58OEaOHInly5ejpqZGumqIiIiIujeDA0tycjLGjRsnvY+OjgYAzJo1C2vXrkVRURHy8vKk7Q0NDXj++edx7tw52NraYujQofjxxx91+pg6dSpKS0uxePFiqNVqhISEID4+vtVEXKPgKSEiIiKjUwghuvxJD41GA5VKhcrKynY/PTRkyS5U1TcBAHKXTmjXvomIiLozQ36/+SwhIiIikj0GFn14SoiIiMjoGFj06fInzIiIiLo+BhYiIiKSPQYWIiIikj0GFiIiIpI9BhYiIiKSPQYWIiIikj0GFiIiIpI9BhYiIiKSPQYWIiIikj0GFiIiIpI9BhYiIiKSPQYWIiIikj0GFiIiIpI9BhY9+OxDIiIi42NgISIiItljYNFDYewCiIiIiIFFH54SIiIiMj4GFiIiIpI9BhY9eEqIiIjI+BhY9OApISIiIuNjYCEiIiLZY2DRg6eEiIiIjI+BRQ+eEiIiIjI+BhYiIiKSPQYWIiIikj0GFiIiIpI9BhYiIiKSPQYWIiIikj0GFiIiIpI9BhYiIiKSPQYWIiIikj0GFiIiIpI9BhYiIiKSPYMDy4EDBzBx4kT4+PhAoVBg27Zt122/detW3HXXXXB3d4ejoyPCw8Oxa9cunTavvvoqFAqFztK/f39DSyMiIiITZXBgqampQXBwMFauXHlD7Q8cOIC77roLO3fuREpKCsaNG4eJEyfiyJEjOu0GDRqEoqIiaTl48KChpREREZGJsjB0h6ioKERFRd1w++XLl+u8/9e//oXvvvsO33//PYYNG3a5EAsLeHl5GVoOERERdQOdPodFq9WiqqoKLi4uOuuzsrLg4+ODoKAgTJ8+HXl5edfso76+HhqNRmchIiIi09XpgeW9995DdXU1HnroIWldWFgY1q5di/j4eKxatQo5OTm47bbbUFVV1WYfsbGxUKlU0uLn59dZ5RMREZERdGpg+frrr/Haa69h8+bN8PDwkNZHRUVhypQpGDp0KCIjI7Fz505UVFRg8+bNbfYTExODyspKacnPz++wmoUQHdY3ERER3RiD57DcrI0bN2LOnDnYsmULIiIirtvWyckJffv2RXZ2dpvblUollEplR5RJREREMtQpR1g2bNiA2bNnY8OGDZgwYYLe9tXV1Th9+jS8vb07obrrUygUxi6BiIio2zP4CEt1dbXOkY+cnBykpaXBxcUFPXv2RExMDM6dO4f169cDaDkNNGvWLKxYsQJhYWFQq9UAABsbG6hUKgDACy+8gIkTJ8Lf3x+FhYVYsmQJzM3NMW3atPYY45/CU0JERETGZ/ARluTkZAwbNky6JDk6OhrDhg3D4sWLAQBFRUU6V/h8+umnaGpqwlNPPQVvb29pefbZZ6U2BQUFmDZtGvr164eHHnoIrq6uOHToENzd3f/s+IiIiMgEKIQJHELQaDRQqVSorKyEo6Nju/Y9eMkuVNc3AQByl+o/nUVEREQ3xpDfbz5LSA8TyHNERERdHgMLERERyR4Dix68SoiIiMj4GFj04CkhIiIi42NgISIiItljYCEiIiLZY2AhIiIi2WNgISIiItljYCEiIiLZY2AhIiIi2WNgISIiItljYCEiIiLZY2AhIiIi2WNgISIiItljYCEiIiLZY2AhIiIi2WNg0YOPPiQiIjI+BhYiIiKSPQYWPRTGLoCIiIgYWPThKSEiIiLjY2AhIiIi2WNgISIiItljYCEiIiLZY2AhIiIi2WNgISIiItljYCEiIiLZY2AhIiIi2WNgISIiItljYCEiIiLZY2AhIiIi2WNgISIiItljYCEiIiLZY2AhIiIi2WNgISIiItkzOLAcOHAAEydOhI+PDxQKBbZt26Z3n3379uGWW26BUqlE7969sXbt2lZtVq5ciYCAAFhbWyMsLAxJSUmGlkZEREQmyuDAUlNTg+DgYKxcufKG2ufk5GDChAkYN24c0tLSMH/+fMyZMwe7du2S2mzatAnR0dFYsmQJUlNTERwcjMjISJSUlBhaHhEREZkghRBC3PTOCgW+/fZbTJ48+ZptFi1ahB07diAjI0Na9/DDD6OiogLx8fEAgLCwMIwYMQIfffQRAECr1cLPzw/PPPMMXnzxRb11aDQaqFQqVFZWwtHR8WaH06aBi+NR29AMAMhdOqFd+yYiIurODPn97vA5LImJiYiIiNBZFxkZicTERABAQ0MDUlJSdNqYmZkhIiJCanO1+vp6aDQanYWIiIhMV4cHFrVaDU9PT511np6e0Gg0uHjxIsrKytDc3NxmG7Va3WafsbGxUKlU0uLn59dh9RMREZHxdcmrhGJiYlBZWSkt+fn5HfZZN3/CjIiIiNqLRUd/gJeXF4qLi3XWFRcXw9HRETY2NjA3N4e5uXmbbby8vNrsU6lUQqlUdljNREREJC8dfoQlPDwcCQkJOuv27NmD8PBwAICVlRVCQ0N12mi1WiQkJEhtiIiIqHszOLBUV1cjLS0NaWlpAFouW05LS0NeXh6AltM1M2fOlNo/8cQTOHPmDBYuXIjff/8dH3/8MTZv3oznnntOahMdHY3PPvsM69atw8mTJzFv3jzU1NRg9uzZf3J4REREZAoMPiWUnJyMcePGSe+jo6MBALNmzcLatWtRVFQkhRcACAwMxI4dO/Dcc89hxYoV8PX1xeeff47IyEipzdSpU1FaWorFixdDrVYjJCQE8fHxrSbiEhERUff0p+7DIhcdeR+WAa/E42Ij78NCRETU3mR1HxYiIiKiP4uBhYiIiGSPgYWIiIhkj4GFiIiIZI+BhYiIiGSPgYWIiIhkj4GFiIiIZI+BhYiIiGSPgYWIiIhkj4GFiIiIZI+BhYiIiGSPgYWIiIhkj4GFiIiIZI+BRQ+BLv8wayIioi6PgYWIiIhkj4FFDwUUxi6BiIio22Ng0YOnhIiIiIyPgYWIiIhkj4FFD54SIiIiMj4GFj14SoiIiMj4GFiIiIhI9hhYiIiISPYYWIiIiEj2GFiIiIhI9hhYiIiISPYYWIiIiEj2GFiIiIhI9hhYiIiISPYYWIiIiEj2GFiIiIhI9hhYiIiISPYYWIiIiEj2GFj0EHz2IRERkdExsOihZWIhIiIyOgYWPRqbLweWCzUNRqyEiIio+7qpwLJy5UoEBATA2toaYWFhSEpKumbbO+64AwqFotUyYcIEqc2jjz7aavv48eNvprQOlfB7ibFLICIi6pYsDN1h06ZNiI6ORlxcHMLCwrB8+XJERkYiMzMTHh4erdpv3boVDQ2Xj0yUl5cjODgYU6ZM0Wk3fvx4fPHFF9J7pVJpaGlERERkogw+wrJs2TLMnTsXs2fPxsCBAxEXFwdbW1usWbOmzfYuLi7w8vKSlj179sDW1rZVYFEqlTrtnJ2db25EREREZHIMCiwNDQ1ISUlBRETE5Q7MzBAREYHExMQb6mP16tV4+OGHYWdnp7N+37598PDwQL9+/TBv3jyUl5dfs4/6+npoNBqdhYiIiEyXQYGlrKwMzc3N8PT01Fnv6ekJtVqtd/+kpCRkZGRgzpw5OuvHjx+P9evXIyEhAW+//Tb279+PqKgoNDc3t9lPbGwsVCqVtPj5+RkyDCIiIupiDJ7D8mesXr0aQ4YMwciRI3XWP/zww9LrIUOGYOjQoejVqxf27duHO++8s1U/MTExiI6Olt5rNBqGFiIiIhNm0BEWNzc3mJubo7i4WGd9cXExvLy8rrtvTU0NNm7ciMcee0zv5wQFBcHNzQ3Z2dltblcqlXB0dNRZOoOiUz6FiIiIrmZQYLGyskJoaCgSEhKkdVqtFgkJCQgPD7/uvlu2bEF9fT0eeeQRvZ9TUFCA8vJyeHt7G1IeERERmSiDrxKKjo7GZ599hnXr1uHkyZOYN28eampqMHv2bADAzJkzERMT02q/1atXY/LkyXB1ddVZX11djQULFuDQoUPIzc1FQkICJk2ahN69eyMyMvImh0VERESmxOA5LFOnTkVpaSkWL14MtVqNkJAQxMfHSxNx8/LyYGamm4MyMzNx8OBB7N69u1V/5ubmSE9Px7p161BRUQEfHx/cfffdeOONN4x+L5ZmLW/LT0REJAcKIbr+w3I0Gg1UKhUqKyvbdT5LXWMz+r8SL71/9NYAvHrvoHbrn4iIqDsz5PebzxK6DgszBR4bEyi9X/trrvGKISIi6sYYWK7DwtwMUYOvf/UTERERdTwGFj04jYWIiMj4GFj04MRbIiIi42Ng0UPb9eckExERdXkMLHowsBARERkfA4sePCNERERkfAwsemiZWIiIiIyOgUUPTrolIiIyPgYWPa6ew2ICNwYmIiLqchhY9Lj6AMu+zFLjFEJERNSNMbDocfURlu/TC41UCRERUffFwKLH1YHFTKEwUiVERETdFwOLHlefEjJjXiEiIup0DCx6XH1Z8+bkAiNVQkRE1H0xsOjh52Lbat3R/IrOL4SIiKgbY2DRI9TfudW6Yk2dESohIiLqvhhYbsC6v43Uec+Jt0RERJ2LgeUGDPJx1Hlvxv9qREREnYo/vTfg6pvbNjbzbrdERESdiYHlBrjZW+m8r2tsNlIlRERE3RMDyw1QKBQYFeQiva9v1BqxGiIiou6HgeUGzRkTJL2ubWgyYiVERETdDwPLDYoY6Ikpob4AgMziaiNXQ0RE1L0wsBjgzgEeAICMc5VGroSIiKh7YWAxQC93ewDAmdJqiKsvHSIiIqIOw8BigJ6utjBTADUNzSipqjd2OURERN0GA4sBlBbm0rOFTpdyHgsREVFnYWAxUJCbHQDgTGmNkSshIiLqPhhYDBT0xzyWnDIGFiIios7CwGKgnn+cElp9MAffHikwcjVERETdAwOLgXydbaTXz206iosNvE0/ERFRR2NgMdDIQBed95NX/mKkSoiIiLoPBhYDOVhb6rzPLK4yUiVERETdx00FlpUrVyIgIADW1tYICwtDUlLSNduuXbsWCoVCZ7G2ttZpI4TA4sWL4e3tDRsbG0RERCArK+tmSusUf789SH8jIiIiajcGB5ZNmzYhOjoaS5YsQWpqKoKDgxEZGYmSkpJr7uPo6IiioiJpOXv2rM72d955Bx988AHi4uJw+PBh2NnZITIyEnV1dYaPqBO8GNVf5/23RwpQyhvJERERdRiDA8uyZcswd+5czJ49GwMHDkRcXBxsbW2xZs2aa+6jUCjg5eUlLZ6entI2IQSWL1+Ol19+GZMmTcLQoUOxfv16FBYWYtu2bTc1qI6mUChgaa6Q3j+36ShGvPUjVu7NNmJVREREpsugwNLQ0ICUlBRERERc7sDMDBEREUhMTLzmftXV1fD394efnx8mTZqE48ePS9tycnKgVqt1+lSpVAgLC7tmn/X19dBoNDpLZ9v4eHirde/uyuz0OoiIiLoDgwJLWVkZmpubdY6QAICnpyfUanWb+/Tr1w9r1qzBd999hy+//BJarRa33norCgpa7mFyaT9D+oyNjYVKpZIWPz8/Q4bRLkL9nfHJjNBO/1wiIqLuqMOvEgoPD8fMmTMREhKCsWPHYuvWrXB3d8cnn3xy033GxMSgsrJSWvLz89ux4ht31wDPVuue33wUT3+dinMVF41QERERkWkyKLC4ubnB3NwcxcXFOuuLi4vh5eV1Q31YWlpi2LBhyM5ume9xaT9D+lQqlXB0dNRZjMHMTIEfo8fqrPsmtQDb04sQvSnNKDURERGZIoMCi5WVFUJDQ5GQkCCt02q1SEhIQHh46zkdbWlubsaxY8fg7e0NAAgMDISXl5dOnxqNBocPH77hPo2pt4c9lj0U3Gr9icLOn1dDRERkqgw+JRQdHY3PPvsM69atw8mTJzFv3jzU1NRg9uzZAICZM2ciJiZGav/6669j9+7dOHPmDFJTU/HII4/g7NmzmDNnDoCWK27mz5+PN998E//73/9w7NgxzJw5Ez4+Ppg8eXL7jLKD3X+LL6aN1J1HIwAcK6jEZwfOoFkrjFMYERGRibAwdIepU6eitLQUixcvhlqtRkhICOLj46VJs3l5eTAzu5yDLly4gLlz50KtVsPZ2RmhoaH49ddfMXDgQKnNwoULUVNTg8cffxwVFRUYM2YM4uPjW91gTs6e/ksfbEi6PJemur4JEz86CABwtLHA1BE9jVUaERFRl6cQQnT5f/5rNBqoVCpUVlYabT4LANzx7l7klte2Wj8z3B+vTxpshIqIiIjky5Dfbz5LqB0tfWBom+vrG7V46utU3P/xL2hq1nZyVURERF0fA0s7GhXkijP/ugdzxgTqrN+UnI8d6UVIzavAcU7GJSIiMhgDSzszM1O0etbQlZq0As1aARM4E0dERNRpGFg6gIW5GUYGurS5rbahCWPf3Ysnvkzp5KqIiIi6LgaWDvLVnDCM6e3Wav2r/zuOggsXset4cRt7ERERUVsYWDqIpbkZvpwThjcmDdJZf7q0RnpdoqnDvR8dxKbf8jq7PCIioi6FgaWDzQgPuOa2N3acRHpBJRZ9c6zzCiIiIuqCGFiM6PujhTrv1ZV1nIxLRETUBgaWTnDs1bvRx8P+um3WHMzBqNgEfJCQ3UlVERERdR28020nKrhQizFv79XbLvnlCCTnXkDEAA9YmDNTEhGRaeKdbmXK1U55Q+0mr/wFT3yZgi9+ye3YgoiIiLoIBpZOZG15Y/+5Cy5cBAD8kFHUkeUQERF1GQwsnUihUOD4a5F46Z4BN7xPTlkN9p8q7cCqiIiI5I+BpZPZKS0w9/YgLJ8aoretADDuvX2YtSYJR/IuQMtb+hMRUTfFwGIkCoX+NkfyKqTXR/MrMGnlL3gwLpGhhYiIuh0LYxfQXd010NOg9kWaOhw7VwkAqK5vgoO1ZUeURUREJEs8wmIktlYWyIm9B7lLJ2BkQNsPSrzSkbMV0mutFqhvauaRFiIi6jYYWIxI8cd5oVG9XPW2Tco9L70uqKjFkFd34+kNRzqsNiIiIjlhYJGBJ+/oZVD7178/gYYmLXakF+FMaTUmfPAzfjjGS6CJiMh08U63MlFd34RzFy4icvkBg/YL9XdGytkLAIDcpRPQ1Kzl3XGJiKhL4J1uuyB7pQX6eTkYvF/lxUbp9X9TCjBwyS7et4WIiEwOA4vMvDzhxm8qBwC5ZTXS6xe2HEVDkxZz1yejtKoeP2eVcmIuERGZBJ4SkqESTR08HK0R8OKOm9rf0lwBpYU5quubEPfILRg/2LudKyQiIvrzDPn95n1YZMjD0fpP7S9Ey5wYAPjp9xIoFAqUaOowIzygHaojIiLqfAwsJqhJq3vQ7O//SQEAjApyRR9Pw+fJEBERGRvnsMjYjn+MAQC8PmnQTfdx5Qm/0up6nC2vQXl1/Z8tjYiIqFMxsMjYIB8VcpdOwMw/cSpnb2aJ9Lq0qh5j392H0Dd/bIfqiIiIOg8DSxdjYXYDT028Qll1g/T6eKFGen2hpgHTPj2E/6YUtFttREREHYWBpYvYMHcU3rpvMOLn33bTfXx64Iz0ekVCFhLPlOOFLUchhMDp0mpotV3+gjEiIjJRDCxdRHgvV0wP80d7XYR+ofbykZdPDpzBnf/ej9e3n2ifzomIiNoZA0sX46X6c5c8X3LllURvx/8OAFj7ay4AoKFJ2y6fQURE1F4YWLoYB2tL7H3hDhxcNA7Lp4bcdD/7My/fvv/KozYf/ZSFvi//gOQrng5NRERkbAwsXVCgmx18nW1xb7DPTfdx6cZyV3tv9ykAwCvfHcf6xFzMXZ+M+qbmm/4cIiKi9sAbx3VhZmYKfPR/w7DmYA5S8yratW8hBBZ/dxwAsDX1HIo1ddAKIPquvu36OURERDeCR1i6uL8O9cHWJ0djSqhvu/Z75WmicxcuYvmPWfggIQsVtQ3ILqlGZW3jtXcmIiJqZzcVWFauXImAgABYW1sjLCwMSUlJ12z72Wef4bbbboOzszOcnZ0RERHRqv2jjz4KhUKhs4wfP/5mSuu2Xpk4EI/fHoTVs4a3S3+ZxVXS60bt5Um4Gec0iFi2H2GxLTefq6xt5BOhiYiowxkcWDZt2oTo6GgsWbIEqampCA4ORmRkJEpKStpsv2/fPkybNg179+5FYmIi/Pz8cPfdd+PcuXM67caPH4+ioiJp2bBhw82NqJtytLbEP+8ZgDsHeLZ73/EZaun1z1ktk3XrGrXYf6oUwa/vxivfZUAIgZyyGoYXIiLqEAYHlmXLlmHu3LmYPXs2Bg4ciLi4ONja2mLNmjVttv/qq6/w5JNPIiQkBP3798fnn38OrVaLhIQEnXZKpRJeXl7S4uzsfHMjIqhsLNu1v7PltdLrK+PIe7syAQBfHsrDWztOYtx7+xC3/wzyz9fi2yMFaOaN6IiIqJ0YFFgaGhqQkpKCiIiIyx2YmSEiIgKJiYk31EdtbS0aGxvh4uKis37fvn3w8PBAv379MG/ePJSXl1+zj/r6emg0Gp2FLvsxeiwA4L5hPdq97yvvlqu44ikBnx/MAdByT5fb3tmL5zYdxabf8tv984mIqHsyKLCUlZWhubkZnp66px08PT2hVquvsZeuRYsWwcfHRyf0jB8/HuvXr0dCQgLefvtt7N+/H1FRUWhubvty2tjYWKhUKmnx8/MzZBgmz91BidylE/D+n7hPy40oq7r+U58Tz5Tjh2NF+PTA6Q6tg4iITF+nXta8dOlSbNy4Efv27YO19eU7tj788MPS6yFDhmDo0KHo1asX9u3bhzvvvLNVPzExMYiOjpbeazQahpZrWBDZD98eOYcvHwvDqNgE/TsYoLCyTm+beV+lAgBGBbkip6wGrnZKjOnj1q51EBGR6TPoCIubmxvMzc1RXFyss764uBheXl7X3fe9997D0qVLsXv3bgwdOvS6bYOCguDm5obs7Ow2tyuVSjg6Ouos1LanxvXGj9Fj4eGg7PTPvnICbsrZC3h2YxoeWX0YDU1aLNtzCsm553Gu4iKe33wUJ/54kjQn7RIRUVsMCixWVlYIDQ3VmTB7aQJteHj4Nfd755138MYbbyA+Ph7Dh+u/7LagoADl5eXw9vY2pDy6DjMzhf5G7Wx7epH0Wq25fDTmy0Nn8UFCFh6MS8TTX6fim9QCTPjwZ2z+LR8j3kpAxrnKTq+ViIjkzeBTQtHR0Zg1axaGDx+OkSNHYvny5aipqcHs2bMBADNnzkSPHj0QGxsLAHj77bexePFifP311wgICJDmutjb28Pe3h7V1dV47bXX8MADD8DLywunT5/GwoUL0bt3b0RGRrbjUOnE65HYmnoODU3aTn8yc4nm8nyXrJLL93jJVLe8FgJY+E06AOC5TWmIGOiJ+Aw1tj01GurKOjQ2azG4h6pTayYiIvkwOLBMnToVpaWlWLx4MdRqNUJCQhAfHy9NxM3Ly4OZ2eUDN6tWrUJDQwMefPBBnX6WLFmCV199Febm5khPT8e6detQUVEBHx8f3H333XjjjTegVHb+aQxTZmtlgUdG+eNcxUUpsLjaWaG8pqHDP/vbI5fvu3PkiscItHXcp7FZi1X7Wibqrv0lF+//2PJ8o2Ov3o2K2kY421nBXsmnShARdScKYQKTBjQaDVQqFSorKzmf5QbllNVAZWOJhf9Nx48ni/Xv0EHslRatHsTo72or3fvl72OD8Mn+lkup1/9tJGauSYKdlTmOvRqJxDPlGNxDBXulBbJKqtDXwwHfHT2Hrw/nYeX0W+DhYN3q84iISD4M+f3mP1O7qUA3OwDAY2MCjRpY2npq9JU3qrvYcPnS9oPZZQCAmoZmfJ2Uh5e3ZaC3hz1u6+OGL37JxVPjemHl3pYjM+/EZ+K9KcHSvr+eLoO90gJDeqiQcLIE/b0d4Ots21HDIiKidsbA0s2F93LFqum3IKOwEp8dyEFDs1b/Tp1ofeLZNtf/L60QAJBdUo3skmoAkMIKAFTUNkJdWYfzNQ1wc7DC/312GADw6YxQPP6fFABA7tIJHVk6ERG1Iz6tmRA1xBsLIvtj099HGbuU67ryLrtJueev2/ZcxUWMik3APR/8jOTcC9L6Q2cu75dXXos565KRrKcvIiIyPs5hIR0lVXWoqW/GuPf2GbuUDvFgqC/+m1IAAAj2c8LR/AoAPNpCRGQMhvx+8wgL6fBwsEagmx02Pd5ytOW1ewcZuaL2dSmsAEDB+Sse6igESv941IBWK9qcW0NERMbDwEJtCgtyRe7SCZh1awAiBrRcsj7vjl5Grqp9Xfnwxje2n8SIt37Ed2nnMGd9MgYv2YXcshqUVtWjqPKi8YokIiIAPCVEN0irFVAogMCYncYupUN5OipR/MdN7p4Y2wtx+1sm8ma8Fqlz75fSqnq42lkZ5Q7CRESmgqeEqN2ZmSmgUCiQ9NKdePKOXnhouK+xS+oQxVfckbfxiiumTpdUI/L9A/jnt8fwa3YZRrz1I57ZeAQA0NAkryuriIhMES9rJoN4OFhj4fj+yCmrwebkAv07dGGrD+ZIr7enFyKzuAqZxVUouNByimhHehEiBxXiHxuOYMXDIQCAdb/m4uPpofB0VKKxWcDKgv8mICJqDzwlRDetvLoejjaWOHzmPB5ZfdjY5cjGhKHeqG9sRlLOefy86C/Yl1kCWysL3DXQEzX1TbC1MgcAnCjSoI+HA0MNEXVbhvx+M7BQu7jY0AwbK3N8k1KA57ccRW8Pe+mGbt1ZTFR/xP7wOwDgwIJxuP3dvYgY4IExvd3w6vcnMH6QF16M6o+tqQWYPToQOzOK8P3RQnw6czgOnCrFhdpGzBjlb+RREBF1DAYWMqpmrYC5mQKjl/6EcxW8wuaSfp4OyCxueTq1h4MSJVX1OtvvGuiJPSdaHpPwzF9648OfsgEA+164AzszihDkZoewQFcs+O9RPBjqi9v6uCPh9xLc0c8djtaWUj8/Z5XiZJEGc28LgkLBScFEJF98lhAZlfkfV84cWDgOi7/LwMhAFzy7Mc24RcnApbACoFVYASCFFaDl0QKXbPwtX7pa6ZFRPfHjyRL8eLIEk0J88F1aIcb1c8fM8AB8eegsYh8YghmrkwAAfT0d4GRrBXOFAkN8VR01LCKiTsEjLNQp1ifmYvF3x/H0uN74aG/LkYPb+7rjwKlSI1dmuhZE9sO7uzIBAFlvRWF/Zil6edgj0M0ONfVNsPvjMm1NXaN0hEarFWgWAmYKhRQ8iYg6Ck8Jkax98UsOfJ1t4WJniQdWJQIA7hvWA98eOQcPByW8VNZIL6g0cpWm61KQCQt0weGclucoLZ8aglPFVVifeFa6y++MUf54fdIg5J2vhb3SAi52VjiQVYaB3o5wd1CisVkLS3NOGCaim8fAQl1GiaYOrvZKCCFwtKACQ3o4wdJcgac3HMFgHxXejv/d2CXSH/p62uNUcctE6ogBnvjxZDHemxKMsEAX7DtViimhvrC2NIdWK/DCf49ioLcj5twWZOSqiUjOGFjIZDwUl4ik3PNQWpihnjdok71LZ5G0f3yrvHbvIGxOzsetvVzxtzGBSC+oRA8nG/T3coCFuRmEEB06Mbij+yeiP4eBhUxKekEF/F3tsCO9CB/vy8ba2SMQsexAq3YfT78FT36VaoQK6c9YENkPP2QUobFJ4Jk7eyNTXQUPByVmhAcg5ex5OFhbwkyhQICrLeqbtEg+ewEhvk5Q2V6+MupgVhnO1zbg3mAfAEDBhVrkna/F85uPYka4Px4bE4iknPPwcmx5uKfFnzyV1disRUlVPXo42fypfoi6OwYWMnmVtY1IPnseIwNd8E58Jno42+DvtwfhrvcPQHOxsc2rcIb1dMKRvIrOL5Y6hLuDEqVV9fhLfw/89HsJgJZLw/t7OUiXhLfFzd4KrnZKNGm1GOrrhIILtXjmL32QU1YDMzOFdN+bM6XVsLEyh6W5GVztrKBQKNDUrIWFuZl05O+/T4RjeIALhBBoaNZCaWFu0Bgu3QKAqLtiYKFuq1krIITA+doGhP0rAU+M7QV/F1ucPV+Lv98ehJDX9xi7RJK5sX3dkV5QgQtXXFoOXPuqtg+mDcMb20+gqq4RY3q7QaFQ4IFbemDHMTUmDPHGsJ5OWLXvNGaE+0OrFfj+aCEeCPVF3P4z2HOiGHueux3OdlYAdE9hVdY24r3dmbj/lh4I8XNCwYWL8HW24SkuMikMLETXELP1GDYk5WHh+H54Jz7T2OUQAQCmhPpiS0oBfJ1t4OlojZSzF3S2339LD2xNPYeXJwxAPy8HfH+0EIsnDkJSTjnyymvxyCh/HC/UoK6xGb4utrjY0AQfJxtU1zVh5d5s9Pawh6ONJb4+nIfFEwfC18kWueU1sDBXYENSHmrqmxF7/xA89EkiRga44NHRAZj3ZSr+NiYA9w1r+0GniafLcba8Bg+P7Kl3fA1NWmSqqzDIx7HTnnBuyPylg1llcHOwQn8v/n5cqbFZi9Ol1ejn6dBhQZmBhegahBAorKyT5h5cbGjGL9llmLM+GUN9VXhklD8W/jcdVuZmaGjmJF/qPoLc7HCmrKbV+venBuP9PVlY+sAQWFua4/ujhZgwxBsPxrXckmB+RB+kF1TC1c4K704JRlZxFbydbGBlboYDp0oxMsgFMd8cw45jRXh5wgCE93JF6tkLmB7mj7Pna1HX2IwB3i3f26VV9bhQ24B34n/H/bf44va+7kg4WYy/9PdAaVU99mWW4v/CeqKqrgnlNfWtAsaBUy33GiqtqsesNUl4Mao/Rvdyw39T8vHo6EAUVlzEoTPlmD06EOXV9ahtaEaTViBi2X4AQO7SCdBqRaeFqpuh1QqU1zTgcE45mrUC9wb7oLSq5blujc1afLQ3G/cG+8DWygJf/JKDv4/tBQ8HJdSVdfBzsW2zTyEEdhwrwpAeKnirbHDoTDmGBzjj+c1H8UOGGgCw+e/hGBno0u7jYWAhMlB9U7M0/6CxWQtzhQIf7c3Gsj2nsPT+IXhx6zEjV0gkf38bHYg1v+TAzV6JB0N9pTs06/P0uN5IyjmPpNzzBn3eXQM94WRjCRc7K9ze1x3TP295CKuFmQJN2mv/tL39wBAs+qbl73Ts/UMQ88ff770v3IFJHx3E7NGBeO6uviirroebvRIVtQ04kleB2/u6QwGgrqkZtlYWSMo5DzMFEOrvjC9+ycVQXxWGB+j/Ud9zohhqTd01nxNWVHkRH+89jaG+KoT6OyMtvwLj+nmgWQg8FJfYZrC8mpu9FcqqGxDi5wRbK3P8erocG+aOgrfKGuU1Leunf34Ivs62uL2vO/6x4YjePnOXTtDbxlAMLETtbM+JYuw+rsY/7xmAN3echKauETPD/aXb4E8d7odNyflGrpKI/oyRAS5SaHpouC82JxfobH8uoi9+yz2PX06XISF6LP7y75YjMx9OG4Zn/vjBPxRzJz78KQtTR/ihrLoeL3+bgTsHeKKvpz0OZJXh3w8FY+iruwEAu5+7Hct2n4Kfiw2evKM3Xt9+AvsyS1rNn5ILBpZ2wMBCxlJd3wSlhRkszBTYnl4EV3srnCjU4M0dJxET1R9fJ+XhbHktpo30w4YkBhoiU/T47UH49MAZg/fzUVmjsLKuAyrqGAws7YCBheRKCIEmrYCluRkST5cju7QaU0J9seibdNTUN+ONyYMw9p19CHK3w0sTBmDG6iTcG+yD0b1dseibY7hniBd2Hms5h6xQAF3/bysRdVXHXr0bDlc8Gb49MLAQmYBLVzkcPlOOs+drMSXUF0k553HozHncN6wHbn93LwBg21OjMXnlL+jlboewIFd8fTgPM0b54z+HzgKAzjODiIhuVkdMvGVgIeomrn4AoRAC1fVNcLC2xP5TpThdUo2/jQnE6oM5eCf+d3w+azhW/JiFIHc7/F+YPyav/AUAsOLhEDy7MU2n7z4e9sgqaXl20ABvR5ws0nTauIhInk69GQUri/Z76CkDCxEZLKesBuYKBbydrHGhpgEejtYAgAs1DXCytcTqgzk4dOY8Fv91IP69JxNhga7o4WyDWWtaJh7/e0ownt9yVOceN1dOYrzyjrRE1DW19zwWBhYi6jRCCAgBnXtXVNU14kJNI3q62iKnrAY19U0Y3EMlXc4ZHuSC6Z8fxpAeKiyZOAiLvknH9DB/OFhbYOaaJMwK94eNlQXi9p/GOw8OxcL/pgMAAlxtkVtee0N1OdlaouKPqy2e+Uvv696un4huDAPLn8TAQmTaLtQ0oFGrhYeDNSprG5FdWo1bejqhWFMPT0cl6pu0yCquxuAejlAoFNIzei7UNAAAnO2skHL2PKwtzdHL3R6f/3wG/b0cMbq3G75PL8Rwf2ccLajAc5uO4vOZw/HjyWJs/C0f6/82EtvTC5FeUIlPZoTi/o9/haauEZ/OGI7Za3+Dg9ICb943GM9uTMPMcH/UN2qxKTlfmlcEACobS1RelOdlqkSGMDdT4PS/7mnXPhlYiIiM5NKdUoUQ0FxsgsrWEhW1DTh2rhLhQa44V3ERXiprKC3MkX++5WiRtaU51v6ag6nDe6Ko8iK2pBTgpXsGIDXvAvafKkX0XX1x7FwlNicXwF5pgQOnSjGmtxtejOqPF7Ycxbj+HrA0V+B/RwsRfVc/fJNagK8P52Hz38PxyR83bwsNcJZO1U0P64mvDufh9r7uqGtsRlLOeViYKdDPywHHCzVGfXQFr4aTD3OzlvB/yeuTBmFmeEC7fgYDCxERtVJR2wCVjWWr58LUXLqf0BUTuCsvNmJ7eiHuGewNlY0lymrqpSNc5TX18He1wzMbUjHIR4WnxvXG7uNq9PV0gJ+LLQ7nlCPY1wk5ZTVYfTAHz9/dF/nnLyL/fC0eDPVFVkk16hqbEeznhGJNHWytzHEwqwwZhZV44e5++PLQWZiZKTA9zB8//V6MyouNuKOvB1YkZOH2vm64rY87nvwqFQO8HTF+kBfmrPsNj90WhPiMIvyWewH339IDt/R0houdFZxsLfF/nx3G+EFemDzMB098mYogNzuMDHTBxt/y4WpnhbsGemLjb/m4b1gPfHvk3E39tx3o7YgTRRr8dag3tqcXAQDslRaorm8CAAS62SHnjzvUvnB3X7y3+xSe+UtvfH+0ELnltdLzogBgQWQ/vLurJTC+PmkQFn93HD2cbDB7dADe3HESr/x1IN7YfgIA8MpfB+L4uUr8kKFG8ssRyCmrQWlVPWytzHGiSIOTRRpMCukh3QX4ibG9sD29EBcbmnFbHzekn6vEE2N7ob6xGd+nF+GjacPw3u5MhPg54+5Bnnj52ww8NMIX4/p5dMjzhBhYiIioW6prbIa1pbnOums9H6ihSQsLM0WrberKOng6KtHYLGBproBCoUBhxUVcbGxGL3d7/JJdBgAY3dsNu46r0cvdDr09HKT9NXWNqG/Uwt1BCeDyLQqatQL1f9zW/9Jpy8ZmLarqmuBiZwVNXSOEaDmN+MOxIgS526O3hz0OZpch2FcFJ1srXKhpgLOdFc6UViPxTDmmDveDhbmZ3oc9Vtc3oaquEd4qG1z62ZfDk78N+f2+qWuTVq5ciYCAAFhbWyMsLAxJSUnXbb9lyxb0798f1tbWGDJkCHbu3KmzXQiBxYsXw9vbGzY2NoiIiEBWVtbNlEZERN3Y1WEFwDUfZmhlYdbmNi+VNRQKBawszKQfdR8nG/RytwfQElRG93YDAEQO8tIJKwDgaG0phRXgcjAwN1PA1spCeg0AluZmcLGzkvZT2bTcmC1qiDf6eTnA3EyBsX3d4WTb0sb5j7ZB7vaYHuYvHRXTFz7slRbwVtlIbeUQVgxlcGDZtGkToqOjsWTJEqSmpiI4OBiRkZEoKWn7csVff/0V06ZNw2OPPYYjR45g8uTJmDx5MjIyMqQ277zzDj744APExcXh8OHDsLOzQ2RkJOrqus4ti4mIiKjjGHxKKCwsDCNGjMBHH30EANBqtfDz88MzzzyDF198sVX7qVOnoqamBtu3b5fWjRo1CiEhIYiLi4MQAj4+Pnj++efxwgsvAAAqKyvh6emJtWvX4uGHH9ZbE08JERERdT0ddkqooaEBKSkpiIiIuNyBmRkiIiKQmJjY5j6JiYk67QEgMjJSap+TkwO1Wq3TRqVSISws7Jp91tfXQ6PR6CxERERkugwKLGVlZWhuboanp6fOek9PT6jV6jb3UavV121/6X8N6TM2NhYqlUpa/Pz8DBkGERERdTHt90CAThQTE4PKykppyc/PN3ZJRERE1IEMCixubm4wNzdHcXGxzvri4mJ4eXm1uY+Xl9d121/6X0P6VCqVcHR01FmIiIjIdBkUWKysrBAaGoqEhARpnVarRUJCAsLDw9vcJzw8XKc9AOzZs0dqHxgYCC8vL502Go0Ghw8fvmafRERE1L1YGLpDdHQ0Zs2aheHDh2PkyJFYvnw5ampqMHv2bADAzJkz0aNHD8TGxgIAnn32WYwdOxb//ve/MWHCBGzcuBHJycn49NNPAbRcDz5//ny8+eab6NOnDwIDA/HKK6/Ax8cHkydPbr+REhERUZdlcGCZOnUqSktLsXjxYqjVaoSEhCA+Pl6aNJuXlwczs8sHbm699VZ8/fXXePnll/HPf/4Tffr0wbZt2zB48GCpzcKFC1FTU4PHH38cFRUVGDNmDOLj42Ftbd0OQyQiIqKujrfmJyIiIqPo8FvzExEREXUmBhYiIiKSPQYWIiIikj2DJ93K0aVpOLxFPxERUddx6Xf7RqbTmkRgqaqqAgDeop+IiKgLqqqqgkqlum4bk7hKSKvVorCwEA4ODlAoFO3at0ajgZ+fH/Lz803yCiRTHx9g+mM09fEBpj9Gjq/rM/UxdtT4hBCoqqqCj4+Pzi1R2mISR1jMzMzg6+vboZ9h6o8AMPXxAaY/RlMfH2D6Y+T4uj5TH2NHjE/fkZVLOOmWiIiIZI+BhYiIiGSPgUUPpVKJJUuWQKlUGruUDmHq4wNMf4ymPj7A9MfI8XV9pj5GOYzPJCbdEhERkWnjERYiIiKSPQYWIiIikj0GFiIiIpI9BhYiIiKSPQYWPVauXImAgABYW1sjLCwMSUlJxi5Jr9jYWIwYMQIODg7w8PDA5MmTkZmZqdOmrq4OTz31FFxdXWFvb48HHngAxcXFOm3y8vIwYcIE2NrawsPDAwsWLEBTU1NnDuWGLF26FAqFAvPnz5fWmcL4zp07h0ceeQSurq6wsbHBkCFDkJycLG0XQmDx4sXw9vaGjY0NIiIikJWVpdPH+fPnMX36dDg6OsLJyQmPPfYYqqurO3sorTQ3N+OVV15BYGAgbGxs0KtXL7zxxhs6zxPpauM7cOAAJk6cCB8fHygUCmzbtk1ne3uNJz09Hbfddhusra3h5+eHd955p6OHBuD642tsbMSiRYswZMgQ2NnZwcfHBzNnzkRhYaFOH3IeH6D//8MrPfHEE1AoFFi+fLnOejmP8UbGd/LkSdx7771QqVSws7PDiBEjkJeXJ2036neroGvauHGjsLKyEmvWrBHHjx8Xc+fOFU5OTqK4uNjYpV1XZGSk+OKLL0RGRoZIS0sT99xzj+jZs6eorq6W2jzxxBPCz89PJCQkiOTkZDFq1Chx6623StubmprE4MGDRUREhDhy5IjYuXOncHNzEzExMcYY0jUlJSWJgIAAMXToUPHss89K67v6+M6fPy/8/f3Fo48+Kg4fPizOnDkjdu3aJbKzs6U2S5cuFSqVSmzbtk0cPXpU3HvvvSIwMFBcvHhRajN+/HgRHBwsDh06JH7++WfRu3dvMW3aNGMMScdbb70lXF1dxfbt20VOTo7YsmWLsLe3FytWrJDadLXx7dy5U7z00kti69atAoD49ttvdba3x3gqKyuFp6enmD59usjIyBAbNmwQNjY24pNPPjHq+CoqKkRERITYtGmT+P3330ViYqIYOXKkCA0N1elDzuPTN8Yrbd26VQQHBwsfHx/x/vvv62yT8xj1jS87O1u4uLiIBQsWiNTUVJGdnS2+++47nd88Y363MrBcx8iRI8VTTz0lvW9ubhY+Pj4iNjbWiFUZrqSkRAAQ+/fvF0K0fLlYWlqKLVu2SG1OnjwpAIjExEQhRMsfbDMzM6FWq6U2q1atEo6OjqK+vr5zB3ANVVVVok+fPmLPnj1i7NixUmAxhfEtWrRIjBkz5prbtVqt8PLyEu+++660rqKiQiiVSrFhwwYhhBAnTpwQAMRvv/0mtfnhhx+EQqEQ586d67jib8CECRPE3/72N511999/v5g+fboQouuP7+ofg/Yaz8cffyycnZ11/owuWrRI9OvXr4NHpOt6P+aXJCUlCQDi7NmzQoiuNT4hrj3GgoIC0aNHD5GRkSH8/f11AktXGmNb45s6dap45JFHrrmPsb9beUroGhoaGpCSkoKIiAhpnZmZGSIiIpCYmGjEygxXWVkJAHBxcQEApKSkoLGxUWds/fv3R8+ePaWxJSYmYsiQIfD09JTaREZGQqPR4Pjx451Y/bU99dRTmDBhgs44ANMY3//+9z8MHz4cU6ZMgYeHB4YNG4bPPvtM2p6TkwO1Wq0zRpVKhbCwMJ0xOjk5Yfjw4VKbiIgImJmZ4fDhw503mDbceuutSEhIwKlTpwAAR48excGDBxEVFQWg64/vau01nsTERNx+++2wsrKS2kRGRiIzMxMXLlzopNHcmMrKSigUCjg5OQEwjfFptVrMmDEDCxYswKBBg1pt78pj1Gq12LFjB/r27YvIyEh4eHggLCxM57SRsb9bGViuoaysDM3NzTr/0QHA09MTarXaSFUZTqvVYv78+Rg9ejQGDx4MAFCr1bCyspK+SC65cmxqtbrNsV/aZmwbN25EamoqYmNjW20zhfGdOXMGq1atQp8+fbBr1y7MmzcP//jHP7Bu3ToAl2u83p9PtVoNDw8Pne0WFhZwcXEx+hhffPFFPPzww+jfvz8sLS0xbNgwzJ8/H9OnTwfQ9cd3tfYaj9z/3F5SV1eHRYsWYdq0adKD8kxhfG+//TYsLCzwj3/8o83tXXmMJSUlqK6uxtKlSzF+/Hjs3r0b9913H+6//37s379fqs+Y360m8bRmurannnoKGRkZOHjwoLFLaTf5+fl49tlnsWfPHlhbWxu7nA6h1WoxfPhw/Otf/wIADBs2DBkZGYiLi8OsWbOMXN2ft3nzZnz11Vf4+uuvMWjQIKSlpWH+/Pnw8fExifF1Z42NjXjooYcghMCqVauMXU67SUlJwYoVK5CamgqFQmHsctqdVqsFAEyaNAnPPfccACAkJAS//vor4uLiMHbsWGOWB4BHWK7Jzc0N5ubmrWY/FxcXw8vLy0hVGebpp5/G9u3bsXfvXvj6+krrvby80NDQgIqKCp32V47Ny8urzbFf2mZMKSkpKCkpwS233AILCwtYWFhg//79+OCDD2BhYQFPT88uPT4A8Pb2xsCBA3XWDRgwQJqtf6nG6/359PLyQklJic72pqYmnD9/3uhjXLBggXSUZciQIZgxYwaee+456YhZVx/f1dprPHL/c3sprJw9exZ79uyRjq4AXX98P//8M0pKStCzZ0/pe+fs2bN4/vnnERAQINXYVcfo5uYGCwsLvd87xvxuZWC5BisrK4SGhiIhIUFap9VqkZCQgPDwcCNWpp8QAk8//TS+/fZb/PTTTwgMDNTZHhoaCktLS52xZWZmIi8vTxpbeHg4jh07pvOX79IX0NV/oDvbnXfeiWPHjiEtLU1ahg8fjunTp0uvu/L4AGD06NGtLkU/deoU/P39AQCBgYHw8vLSGaNGo8Hhw4d1xlhRUYGUlBSpzU8//QStVouwsLBOGMW11dbWwsxM9+vH3Nxc+ldeVx/f1dprPOHh4Thw4AAaGxulNnv27EG/fv3g7OzcSaNp26WwkpWVhR9//BGurq4627v6+GbMmIH09HSd7x0fHx8sWLAAu3btAtC1x2hlZYURI0Zc93vH6L8df2rKronbuHGjUCqVYu3ateLEiRPi8ccfF05OTjqzn+Vo3rx5QqVSiX379omioiJpqa2tldo88cQTomfPnuKnn34SycnJIjw8XISHh0vbL12advfdd4u0tDQRHx8v3N3dZXPZ79WuvEpIiK4/vqSkJGFhYSHeeustkZWVJb766itha2srvvzyS6nN0qVLhZOTk/juu+9Eenq6mDRpUpuXyQ4bNkwcPnxYHDx4UPTp00cWlzXPmjVL9OjRQ7qseevWrcLNzU0sXLhQatPVxldVVSWOHDkijhw5IgCIZcuWiSNHjkhXybTHeCoqKoSnp6eYMWOGyMjIEBs3bhS2tradckns9cbX0NAg7r33XuHr6yvS0tJ0vneuvDJEzuPTN8a2XH2VkBDyHqO+8W3dulVYWlqKTz/9VGRlZYkPP/xQmJubi59//lnqw5jfrQwsenz44YeiZ8+ewsrKSowcOVIcOnTI2CXpBaDN5YsvvpDaXLx4UTz55JPC2dlZ2Nraivvuu08UFRXp9JObmyuioqKEjY2NcHNzE88//7xobGzs5NHcmKsDiymM7/vvvxeDBw8WSqVS9O/fX3z66ac627VarXjllVeEp6enUCqV4s477xSZmZk6bcrLy8W0adOEvb29cHR0FLNnzxZVVVWdOYw2aTQa8eyzz4qePXsKa2trERQUJF566SWdH7euNr69e/e2+fdu1qxZQoj2G8/Ro0fFmDFjhFKpFD169BBLly41+vhycnKu+b2zd+/eLjE+fWNsS1uBRc5jvJHxrV69WvTu3VtYW1uL4OBgsW3bNp0+jPndqhDiiltLEhEREckQ57AQERGR7DGwEBERkewxsBAREZHsMbAQERGR7DGwEBERkewxsBAREZHsMbAQERGR7DGwEBERkewxsBAREZHsMbAQERGR7DGwEBERkewxsBAREZHs/T8WsxuMVlGDiAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
=======
        "id": "AmYVqwCp8HZf"
      },
      "execution_count": null,
      "outputs": []
>>>>>>> dfb859624f482885dfbb3393c14043067cdd492b
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "/PlDLt3cLqbp",
        "id": "NP1ENRqoKDyM"
      },
      "source": [
        "## Conv 2D as Message Passing Neural Network\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "The use of graph is a way to structure data by adding neighborhood information between features. This then allows to do operations on the data that are local to each node and its neighbors. This is the main idea behind Graph Neural Networks (GNNs). [`pytorch-geometric`](https://pytorch-geometric.readthedocs.io/en/latest/) is a library compatible with PyTorch that allows to easily implement GNNs. The most general structure is the [`MessagePassing`](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.MessagePassing.html#torch_geometric.nn.conv.MessagePassing) class that is then used as a base for more specific GNNs as seen in the course ([Graph Convolutional Networks](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCNConv.html#torch_geometric.nn.conv.GCNConv) or [Graph AttenTion Convolution](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GATConv.html#torch_geometric.nn.conv.GATConv)).\n",
        "\n",
        "On the other hand, you already know an operation that uses the structure of the data to do local operations: the convolution (https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html). One can see the convolution as a specific case of the message passing neural network. The goal of this notebook is to show how to use the `MessagePassing` class to implement a convolutional neural network.\n",
        "You will be asked to implement 3 functions. You should give back those three functions in a file named `conv_as_message_passing.py`. These functions will then be automatically tested. So be sure to respect the function signature and the function name.\n",
        "\n",
        "\n",
        "## Assumptions\n",
        "\n",
        "To make the implementation easier we will make some assumptions:\n",
        "- the input is a single image (batch size of 1) of size 'C x H x W'\n",
        "- the convolution will be a 3x3 kernel with stride 1 and padding 1.\n",
        "\n",
        "You may also assume that the Conv2D layer has no bias but it will be slightly penalized in the grading.\n",
        "\n",
        "Bonus points will be given if you can handle the cases that are not covered by those assumptions.\n",
        "\n",
        "\n",
        "## Questions\n",
        "\n",
        "### Question 2\n",
        "\n",
        "> Using the formalism used in the [`MessagePassing`](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.MessagePassing.html#torch_geometric.nn.conv.MessagePassing) documentation (and on [wikipedia](https://en.wikipedia.org/wiki/Graph_neural_network#Message_passing_layers) with sligthly different notations), explain how theorically you can simulate a 2D convolution using the `MessagePassing` formalism. This may include a pre-processing step to transform the image into a graph and then a post-processing step to transform the graph back into an image. (:warning: Those steps should be independent of the parameters of the convolution, but not necessarily from the hyper-parameters.)\n",
        "$$\\mathbf{x}_{i}^{\\prime} = \\gamma_{\\mathbf{\\Theta}}\\left( \\mathbf{x}_{i},\\bigoplus\\limits_{j \\in \\mathcal{N}(i)}\\,\\phi_{\\mathbf{\\Theta}}\\left( \\mathbf{x}_{i},\\mathbf{x}_{j},\\mathbf{e}_{j,i} \\right) \\right),$$\n",
        "\n",
        "\n",
        "HINT : It is possible to do it with the following $\\gamma$ :\n",
        "\n",
        "$$ \\gamma_\\Theta : x,y \\mapsto y $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In a 2D convolution, for the ith-line & jth- column pixels,\n",
        "\n",
<<<<<<< HEAD
        "$$X^{'}_{i,j} = \\sum_{(i', j') \\in \\mathcal{N}(i,j)}W_{i',j'}X_{i+i', j+j'} + b_{i', j'}\\quad  (1)$$  <br>\n",
        "Hence, here we will represent the image as graph where the node are the pixels and the node value will be the pixel value ( Grayscale, RGB, ...). The edges will be set to 1. <br>\n",
        "Therefore, we can write the convolution as a MessagePassing Problem where :\n",
        "$$ \\gamma_\\Theta : x,y \\mapsto y \\quad ; \\quad \\bigoplus\\limits_{j \\in \\mathcal{N}(i)} = \\sum_{j \\in \\mathcal{N}(i)}  \\quad ; \\quad \\phi_{\\mathbf{\\Theta}}(x_i, x_j, e_{i,j}) = k_{i,j} \\left( \\sum_{t = 1}^{C_{in}} W_{i,j}^{(t)} x_j^{(t)} \\right)+ b_{i}$$\n",
        "$$ x^{'}_{i} = \\gamma_{\\Theta}(x_i, k_{i,j} \\left( \\sum_{t = 1}^{C_{in}} W_{i,j}^{(t)} x_j^{(t)} \\right)+ b_{i}) $$\n",
        "We suppose in the pre-processing from image to graph that $x_i$ is in its own neighbourhood $\\mathcal{N}(i)$ and $W \\in \\mathcal{M}_{C_{out}, C_{in}}$ and $b_{i}\\in \\mathbb{R}^{C_{out}}$ and $K \\in \\mathcal{M}_{3, 3}$ in the example\n",
        "\n"
=======
        "$$\\forall k \\in \\{1, ..,  C_{out}\\} \\quad X^{(k)^{'}}_{i,j} = \\sum_{(i', j') \\in \\mathcal{N}(i,j)}W_{i',j'}^{(k)}X_{i+i', j+j'} + b_{i,j}^{(k)}\\quad  (1)$$  <br>\n",
        "We can rewrite it with a tensor notation : <br>\n",
        "$$X_{i,j} = \\sum_{(i', j') \\in \\mathcal{N}(i,j)} W \\otimes x_j + b_{i}$$\n",
        "Hence, here we will represent the image as graph where the node are the pixels and the node value will be the pixel value ( Grayscale, RGB, ...). The edges will be set to 1. <br>\n",
        "Therefore, we can write the convolution as a MessagePassing Problem where :\n",
        "$$ \\gamma_\\Theta : x,y \\mapsto y \\quad ; \\quad \\bigoplus\\limits_{j \\in \\mathcal{N}(i)} = \\sum_{j \\in \\mathcal{N}(i)}  \\quad ; \\quad \\phi_{\\mathbf{\\Theta}}(x_i, x_j, e_{i,j}) = W \\otimes x_j+ b_{i}$$\n",
        "$$ x^{'}_{i} = \\gamma_{\\Theta}(x_i, \\left (\\sum_{j \\in \\mathcal{N}(i)} W \\otimes x_j \\right)+ b_{i}) $$\n",
        "We suppose in the pre-processing from image to graph that $x_i$ is in its own neighbourhood $\\mathcal{N}(i)$ and $W \\in \\mathcal{M}_{C_{out}, 3, 3, C_{in}}$ and $b_{i}\\in \\mathbb{R}^{C_{out}}$  in the example"
>>>>>>> dfb859624f482885dfbb3393c14043067cdd492b
      ],
      "metadata": {
        "id": "vGtUD4NQYI5a"
      }
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 4,
=======
      "execution_count": null,
>>>>>>> dfb859624f482885dfbb3393c14043067cdd492b
      "metadata": {
        "cell_ktag": "/PlDLt3cLqbp",
        "id": "dhmu9sHqKDyM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch_geometric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "/PlDLt3cLqbp",
        "id": "RnNkDwGRKDyM"
      },
      "source": [
        "### Question 3\n",
        "\n",
        "> Implement the pre-processing function, you can use the follwing code skeleton (you may change the output type, it is just a strong suggestion):"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "def image_to_graph(\n",
        "    image: torch.Tensor, conv2d: torch.nn.Conv2d | None = None\n",
        ") -> Data:\n",
        "    \"\"\"\n",
        "    Converts an image tensor to a PyTorch Geometric Data object.\n",
        "\n",
        "    Each pixel is treated as a node with features given by its channel values.\n",
        "    Edges are added between a pixel and all pixels in its receptive field, determined\n",
        "    by a 3x3 neighborhood. If conv2d is provided, its parameters are used for validation.\n",
        "\n",
        "    Arguments:\n",
        "    ----------\n",
        "    image : torch.Tensor\n",
        "        Image tensor of shape (C, H, W).\n",
        "    conv2d : torch.nn.Conv2d, optional\n",
        "        Conv2d layer to simulate, by default None. Used to determine the receptive field.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    Data\n",
        "        Graph representation of the image.\n",
        "    \"\"\"\n",
<<<<<<< HEAD
        "    # Validate image dimensions.\n",
        "    assert image.dim() == 3, f\"Expected 3D tensor, got {image.dim()}D tensor.\"\n",
        "\n",
        "    # If conv2d is provided, ensure its parameters match the expected receptive field.\n",
        "    if conv2d is not None:\n",
        "        assert conv2d.padding[0] == conv2d.padding[1] == 1, \"Expected padding of 1 on both sides.\"\n",
        "        assert conv2d.kernel_size[0] == conv2d.kernel_size[1] == 3, \"Expected kernel size of 3x3.\"\n",
        "        assert conv2d.stride[0] == conv2d.stride[1] == 1, \"Expected stride of 1.\"\n",
        "    # For this implementation, we assume a 3x3 receptive field.\n",
        "    kernel_size = 3\n",
=======
        "    # # Validate image dimensions.\n",
        "    # assert image.dim() == 3, f\"Expected 3D tensor, got {image.dim()}D tensor.\"\n",
        "\n",
        "    # # If conv2d is provided, ensure its parameters match the expected receptive field.\n",
        "    # if conv2d is not None:\n",
        "    #     assert conv2d.padding[0] == conv2d.padding[1] == 1, \"Expected padding of 1 on both sides.\"\n",
        "    #     assert conv2d.kernel_size[0] == conv2d.kernel_size[1] == 3, \"Expected kernel size of 3x3.\"\n",
        "    #     assert conv2d.stride[0] == conv2d.stride[1] == 1, \"Expected stride of 1.\"\n",
        "    # # For this implementation, we assume a 3x3 receptive field.\n",
        "    kernel_size = conv2d.kernel_size[0]\n",
>>>>>>> dfb859624f482885dfbb3393c14043067cdd492b
        "    pad = kernel_size // 2  # pad=1\n",
        "\n",
        "    # Get image dimensions.\n",
        "    C, H, W = image.shape\n",
        "\n",
        "    # Create node features:\n",
        "    # Reshape the image so that each pixel (node) has a feature vector of length C.\n",
        "    # (C, H, W) -> (H*W, C)\n",
        "    x = image.view(C, -1).transpose(0, 1)\n",
        "\n",
        "    # Build edges: each pixel is connected to all pixels in its 3x3 neighborhood.\n",
        "    # We compute the neighbor offsets for a 3x3 grid.\n",
        "    offsets = [(di, dj) for di in range(-pad, pad + 1) for dj in range(-pad, pad + 1)]\n",
        "\n",
        "    src, dst, edge_attr = [], [], []\n",
        "    for i in range(H):\n",
        "        for j in range(W):\n",
        "            current_index = i * W + j\n",
        "            for k, (di, dj) in enumerate(offsets):\n",
        "                ni, nj = i + di, j + dj\n",
        "                # Only add valid neighbors (within image bounds)\n",
        "                if 0 <= ni < H and 0 <= nj < W:\n",
        "                    neighbor_index = ni * W + nj\n",
        "                    src.append(current_index)\n",
        "                    dst.append(neighbor_index)\n",
        "                    # Create one-hot edge attributes to represent kernel elements.\n",
        "                    attr = torch.zeros(len(offsets), dtype=torch.float)\n",
        "                    attr[k] = 1.0\n",
        "                    edge_attr.append(attr)\n",
        "\n",
        "    # Construct edge index tensor.\n",
        "    edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
        "    # Stack edge attributes.\n",
        "    edge_attr = torch.stack(edge_attr)\n",
        "\n",
        "    # Return the graph as a PyG Data object.\n",
        "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)"
      ],
      "metadata": {
        "id": "NByg_asYav_i"
      },
<<<<<<< HEAD
      "execution_count": 9,
=======
      "execution_count": null,
>>>>>>> dfb859624f482885dfbb3393c14043067cdd492b
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "/PlDLt3cLqbp",
        "id": "Qy_KM-RlKDyM"
      },
      "source": [
        "### Question 4\n",
        "\n",
        "> Implement the post-processing function, you can use the follwing code skeleton:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def graph_to_image(\n",
        "    data: torch.Tensor, height: int, width: int, conv2d: torch.nn.Conv2d | None = None\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Converts a graph representation of an image to an image tensor.\n",
        "\n",
        "    Each row of the graph data is assumed to be the node features corresponding\n",
        "    to one pixel of the image. The image is recovered by reshaping the node features\n",
        "    to match the original spatial dimensions.\n",
        "\n",
        "    Arguments:\n",
        "    ----------\n",
        "    data : torch.Tensor\n",
        "        Graph data representation of the image with shape (H*W, C).\n",
        "    height : int\n",
        "        Height of the image.\n",
        "    width : int\n",
        "        Width of the image.\n",
        "    conv2d : torch.nn.Conv2d, optional\n",
        "        Conv2d layer to simulate, by default None. Used for validating expected parameters.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    torch.Tensor\n",
        "        Image tensor of shape (C, H, W).\n",
        "    \"\"\"\n",
        "    # Assumptions: data is a 2D tensor with shape (num_nodes, num_features).\n",
<<<<<<< HEAD
        "    assert data.dim() == 2, f\"Expected 2D tensor, got {data.dim()}D tensor.\"\n",
        "    if conv2d is not None:\n",
        "        assert conv2d.padding[0] == conv2d.padding[1] == 1, \"Expected padding of 1 on both sides.\"\n",
        "        assert conv2d.kernel_size[0] == conv2d.kernel_size[1] == 3, \"Expected kernel size of 3x3.\"\n",
        "        assert conv2d.stride[0] == conv2d.stride[1] == 1, \"Expected stride of 1.\"\n",
        "\n",
        "    num_nodes, num_features = data.shape\n",
        "    assert num_nodes == height * width, \"Mismatch between provided height, width and data shape.\"\n",
=======
        "    # assert data.dim() == 2, f\"Expected 2D tensor, got {data.dim()}D tensor.\"\n",
        "    # if conv2d is not None:\n",
        "    #     assert conv2d.padding[0] == conv2d.padding[1] == 1, \"Expected padding of 1 on both sides.\"\n",
        "    #     assert conv2d.kernel_size[0] == conv2d.kernel_size[1] == 3, \"Expected kernel size of 3x3.\"\n",
        "    #     assert conv2d.stride[0] == conv2d.stride[1] == 1, \"Expected stride of 1.\"\n",
        "\n",
        "    num_nodes, num_features = data.shape\n",
        "    # assert num_nodes == height * width, \"Mismatch between provided height, width and data shape.\"\n",
>>>>>>> dfb859624f482885dfbb3393c14043067cdd492b
        "\n",
        "    # Recover the image by reshaping the node features.\n",
        "    # Original node features were arranged as (H*W, C). We need to produce (C, H, W).\n",
        "    image = data.transpose(0, 1).view(num_features, height, width)\n",
        "\n",
        "    return image\n"
      ],
      "metadata": {
        "id": "nz7EwyiQcB6L"
      },
<<<<<<< HEAD
      "execution_count": 10,
=======
      "execution_count": null,
>>>>>>> dfb859624f482885dfbb3393c14043067cdd492b
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "ref_conv = torch.nn.Conv2d(5, 7, kernel_size=3, padding=1, stride=1)\n",
        "\n",
        "\n",
        "# Load an MNIST image\n",
        "test_dataset = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transforms.ToTensor())\n",
        "image, _ = test_dataset[0]  # Take the first image\n",
        "axes[0].imshow(image.squeeze(), cmap=\"gray\")\n",
        "axes[0].set_title(\"Original Image\")\n",
        "axes[0].axis(\"off\")\n",
        "\n",
        "\n",
        "# Convert image to graph\n",
        "graph = image_to_graph(image, ref_conv)  # Ajoute une dimension de canal\n",
        "\n",
        "image_recomposed = graph_to_image(graph.x, 28, 28, ref_conv)\n",
        "\n",
        "axes[1].imshow(image.squeeze(), cmap=\"gray\")\n",
        "axes[1].set_title(\"Image converted to graph and back to image\")\n",
        "axes[1].axis(\"off\")\n"
      ],
      "metadata": {
<<<<<<< HEAD
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "wTkufJlItSY7",
        "outputId": "d54b0ae0-d241-4a3b-c85e-189d127fd9f8"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.5, 27.5, 27.5, -0.5)"
            ]
          },
          "metadata": {},
          "execution_count": 92
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
=======
        "id": "wTkufJlItSY7"
      },
      "execution_count": null,
      "outputs": []
>>>>>>> dfb859624f482885dfbb3393c14043067cdd492b
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "/PlDLt3cLqbp",
        "id": "vRkXO4B8KDyN"
      },
      "source": [
        "#### Recommended test cases\n",
        "\n",
        "We **encourage** you to test that you have the property that the pre-processing function followed by the post-processing function is the identity function."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 11,
=======
      "execution_count": null,
>>>>>>> dfb859624f482885dfbb3393c14043067cdd492b
      "metadata": {
        "cell_ktag": "/PlDLt3cLqbp",
        "id": "UlooPeQUKDyN"
      },
      "outputs": [],
      "source": [
        "ref_conv = torch.nn.Conv2d(5, 7, kernel_size=3, padding=1, stride=1)\n",
        "image = torch.randn(5, 10, 11)\n",
        "g_image = image_to_graph(image, ref_conv)\n",
        "reconstructed_image = graph_to_image(g_image.x, 10, 11, ref_conv)\n",
        "assert torch.allclose(image, reconstructed_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "/PlDLt3cLqbp",
        "id": "eB4EnJZUKDyN"
      },
      "source": [
        "### Question 5\n",
        "\n",
        "> Implement the `Conv2dMessagePassing` class that will simulate a 2D convolution using the `MessagePassing` formalism.\n",
        "You should inherit from the `MessagePassing` class and only change the `__init__` and `message` functions (the `forward` function has already been changed for you). You should use the following code skeleton:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch_geometric.nn as pyg_nn\n",
        "\n",
        "class Conv2dMessagePassing(pyg_nn.MessagePassing):\n",
        "    \"\"\"\n",
        "    A Message Passing layer that simulates a given Conv2d layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, conv2d: nn.Conv2d):\n",
<<<<<<< HEAD
        "        # We use \"add\" operator to simulate a convolution\n",
        "        super().__init__(aggr=\"add\")\n",
        "\n",
        "\n",
        "        self.kernel = conv2d.weight.detach()\n",
        "\n",
        "        self.in_channels = conv2d.in_channels\n",
        "        self.out_channels = conv2d.out_channels\n",
        "\n",
        "    def forward(self, data):\n",
        "        self.edge_index = data.edge_index\n",
        "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
        "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
        "\n",
        "    def message(self, x_j: torch.Tensor, edge_attr: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Computes the message to be passed for each edge.\n",
        "        For each edge e = (u, v) in the graph indexed by i,\n",
        "        the message trough the edge e (ie from node u to node v)\n",
        "        should be returned as the i-th line of the output tensor.\n",
        "        (The message is phi(u, v, e) in the formalism.)\n",
        "        To do this you can access the features of the source node\n",
        "        in x_j[i] and the attributes of the edge in edge_attr[i].\n",
        "\n",
        "        Arguments:\n",
        "        ----------\n",
        "        x_j : torch.Tensor\n",
        "            The features of the souce node for each edge (of size E x in_channels).\n",
        "        edge_attr : torch.Tensor\n",
        "            The attributes of the edge (of size E x edge_attr_dim).\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        torch.Tensor\n",
        "            The message to be passed for each edge (of size E x out_channels).\n",
        "        \"\"\"\n",
        "        # Reshape edge_attr pour √™tre de taille (E, 3, 3) au lieu de (E, 9)\n",
        "        edge_attr = edge_attr.view(-1, 3, 3) # Convertir en (E, 3, 3)\n",
=======
        "        # On utilise l'agr√©gation de type \"sum\" pour simuler une convolution\n",
        "        super().__init__(aggr=\"add\", flow='target_to_source')\n",
        "\n",
        "        # R√©cup√©rer les poids du noyau de convolution (kernel) sous forme (out_channels, in_channels, kH, kW)\n",
        "        self.kernel = conv2d.weight.detach()  # On ne veut pas mettre √† jour ces poids ici\n",
        "        self.kernel_size = conv2d.kernel_size[0]\n",
        "        # Nombre de canaux d'entr√©e et de sortie\n",
        "        self.in_channels = conv2d.in_channels\n",
        "        self.out_channels = conv2d.out_channels\n",
        "        self.bias = torch.nn.Parameter(conv2d.bias.data if conv2d.bias is not None else torch.zeros(conv2d.out_channels))\n",
        "\n",
        "    def forward(self, data):\n",
        "        \"\"\"\n",
        "        Passe le graphe dans la couche Message Passing.\n",
        "        \"\"\"\n",
        "        self.edge_index = data.edge_index\n",
        "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
        "        propagation = self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
        "        propagation = propagation + self.bias\n",
        "        return propagation\n",
        "\n",
        "    def message(self, x_j: torch.Tensor, edge_attr: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Calcule le message √† passer √† chaque voisin.\n",
        "        x_j : caract√©ristiques des n≈ìuds sources (E, in_channels)\n",
        "        edge_attr : attributs des ar√™tes, indiquant la position dans le noyau (E, 9)\n",
        "        \"\"\"\n",
        "\n",
        "        # Reshape edge_attr pour √™tre de taille (E, 3, 3) au lieu de (E, 9)\n",
        "        E = x_j.shape[0]\n",
        "        edge_attr = edge_attr.view(E, self.kernel_size, self.kernel_size)  # Convertir en (E, 3, 3)\n",
>>>>>>> dfb859624f482885dfbb3393c14043067cdd492b
        "\n",
        "        # Poids associ√©s √† chaque ar√™te (E, out_channels, in_channels)\n",
        "        kernel_weights = torch.einsum(\"oiwh,ehw->eoi\", self.kernel, edge_attr)\n",
        "\n",
        "        # Multiplication des features par les poids du noyau\n",
        "        out = torch.einsum(\"ei,eoi->eo\", x_j, kernel_weights)\n",
        "\n",
<<<<<<< HEAD
        "        return out# (E, out_channels)\n"
=======
        "        return out  # (E, out_channels)\n"
>>>>>>> dfb859624f482885dfbb3393c14043067cdd492b
      ],
      "metadata": {
        "id": "NQXfkFn0Ji1_"
      },
<<<<<<< HEAD
      "execution_count": 125,
=======
      "execution_count": null,
>>>>>>> dfb859624f482885dfbb3393c14043067cdd492b
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "/PlDLt3cLqbp",
        "id": "Wz_PeqnBKDyN"
      },
      "source": [
        "## Test example"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from conv_as_message_passing import image_to_graph, graph_to_image, Conv2dMessagePassing\n",
        "\n",
<<<<<<< HEAD
=======
        "\n",
>>>>>>> dfb859624f482885dfbb3393c14043067cdd492b
        "c = 5\n",
        "h = 10\n",
        "w = 11\n",
        "\n",
<<<<<<< HEAD
        "ref_conv = torch.nn.Conv2d(c, 2, kernel_size=3, padding=1, stride=1, bias=False)\n",
        "image = torch.randn(c, h, w)\n",
        "#ref_conv.weight.data = torch.randn_like(ref_conv.weight.data)\n",
=======
        "ref_conv = torch.nn.Conv2d(c, 2, kernel_size=3, padding=1, stride=1, bias=True)\n",
        "image = torch.randn(c, h, w)\n",
        "ref_conv.weight.data = torch.randn_like(ref_conv.weight.data)\n",
>>>>>>> dfb859624f482885dfbb3393c14043067cdd492b
        "g_image = image_to_graph(image, ref_conv)\n",
        "\n",
        "conv_mp = Conv2dMessagePassing(ref_conv)\n",
        "\n",
        "g_image = conv_mp(g_image)\n",
        "\n",
<<<<<<< HEAD
=======
        "\n",
>>>>>>> dfb859624f482885dfbb3393c14043067cdd492b
        "y_th = ref_conv(image)\n",
        "\n",
        "ref_conv.weight.data = torch.randn_like(ref_conv.weight.data)\n",
        "reconstructed_image = graph_to_image(g_image, h, w, ref_conv)\n",
        "\n",
        "\n",
        "\n",
        "assert torch.allclose(y_th, reconstructed_image, atol=1e-4)\n"
      ],
      "metadata": {
<<<<<<< HEAD
        "id": "iIJXCF32Jp7k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "c913a649-bfc8-40bf-f010-237575d44806"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-129-f43de2da078a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_th\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreconstructed_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_th[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaXp3xLzt9L3",
        "outputId": "294ce952-6520-4357-d0fa-25af60afb590"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0159, -0.2534,  0.4840, -0.1096, -0.4670, -0.8868,  0.2662,  0.5728,\n",
              "        -0.3283, -0.5726, -0.2729], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reconstructed_image[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_muMW245t_ju",
        "outputId": "a8d35b37-125d-480d-d6e2-d36fea6a52c0"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.0044,  0.3123, -0.0980, -0.3104,  0.2729, -0.4052, -0.2811, -0.2942,\n",
              "        -0.5676, -0.3066, -0.0546])"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
=======
        "id": "iIJXCF32Jp7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
>>>>>>> dfb859624f482885dfbb3393c14043067cdd492b
      "metadata": {
        "cell_ktag": "/PlDLt3cLqbp",
        "id": "99tBRvgHKDyN"
      },
      "outputs": [],
      "source": [
        "#from conv_as_message_passing import image_to_graph, graph_to_image, Conv2dMessagePassing\n",
        "\n",
        "c = 1\n",
        "h = 28\n",
        "w = 28\n",
        "\n",
        "# c = 5\n",
        "# h = 10\n",
        "# w = 11\n",
        "\n",
        "image, _ = test_dataset[0]  # Take the first image\n",
        "\n",
        "ref_conv = torch.nn.Conv2d(c, 2, kernel_size=3, padding=1, stride=1, bias=False)\n",
        "#image = torch.randn(c, h, w)\n",
        "ref_conv.weight.data = torch.randn_like(ref_conv.weight.data)\n",
        "g_image = image_to_graph(image, ref_conv)\n",
        "\n",
        "conv_mp = Conv2dMessagePassing(ref_conv)\n",
        "\n",
        "g_image = conv_mp(g_image)\n",
        "\n",
        "\n",
        "y_th = ref_conv(image)\n",
        "\n",
        "ref_conv.weight.data = torch.randn_like(ref_conv.weight.data)\n",
        "reconstructed_image = graph_to_image(g_image, h, w, ref_conv)\n",
        "\n",
        "\n",
        "\n",
<<<<<<< HEAD
        "assert torch.allclose(y_th, reconstructed_image, atol=10)\n"
=======
        "assert torch.allclose(y_th, reconstructed_image, atol=1e-4)\n"
>>>>>>> dfb859624f482885dfbb3393c14043067cdd492b
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot image originale\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Afficher l'image originale (avant la convolution)\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(y_th[0].cpu().detach().numpy(), cmap='gray')  # On affiche la premi√®re couche (canal)\n",
        "plt.title(\"Original Image (First Channel)\")\n",
        "plt.axis('off')\n",
        "\n",
        "# Afficher l'image reconstruite\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(reconstructed_image[0].cpu().detach().numpy(), cmap='gray')  # On affiche la premi√®re couche (canal)\n",
        "plt.title(\"Reconstructed Image (First Channel)\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
<<<<<<< HEAD
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "TAKOIwJwGAph",
        "outputId": "c8d93f99-437a-4733-db8f-da81fdb4549d"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sDE6PVloOxBP"
=======
        "id": "TAKOIwJwGAph"
>>>>>>> dfb859624f482885dfbb3393c14043067cdd492b
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "kfiletag": "/PlDLt3cLqbp",
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
<<<<<<< HEAD
}
=======
}
>>>>>>> dfb859624f482885dfbb3393c14043067cdd492b
