{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "NlwqZF0IUj4b"
   },
   "source": [
    "# TP4 :  Learning on a low budget\n",
    "**Th√©o Rudkiewicz, Cyriaque Rousselot**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "NlwqZF0IUj4b"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "**Context :**\n",
    "\n",
    "Assume we are in a context where few \"gold\" labeled data samples are available for training, say \n",
    "\n",
    "$$\\mathcal{X}_{\\text{train}} = \\{(x_n,y_n)\\}_{n\\leq N_{\\text{train}}}$$\n",
    "\n",
    "where $N_{\\text{train}}$ is small. \n",
    "\n",
    "A large test set $\\mathcal{X}_{\\text{test}}$ exists but is not accessible. \n",
    "(To make your task easier, we provide you with some data (named `test_dataset` in the code) that you can use to test your model, but you **must not** use it to train your model).\n",
    "\n",
    "We also assume that we have a limited computational budget.\n",
    "\n",
    "The goal of this practical session is to guide you through different methods that will help you get better results from few resources (data & compute)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "NlwqZF0IUj4b"
   },
   "source": [
    "In this practical, we will use the `resnet18` architecture. We will use models from the [pytorch vision hub ](https://pytorch.org/vision/stable/models.html#)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "NlwqZF0IUj4b"
   },
   "source": [
    "# QUESTIONS\n",
    "\n",
    "## Grading\n",
    "\n",
    "You will be graded on 5 questions. You will need to provide 7 files : \n",
    "1. This Notebook\n",
    "2. `utils.py`\n",
    "3. `last_layer_finetune.pth` (the file **must be of size less than 5Mo**)\n",
    "4. `daug_resnet.pth` (the file **must be of size less than 50Mo**)\n",
    "5. `final_model.pth` (the file **must be of size less than 50Mo**)\n",
    "6. `drawing_lora.png`\n",
    "7. `cutmix.png`\n",
    "\n",
    "If the code you defined passes all our tests, you will get the full grade. Otherwise we  will look at the intermediate questions in the notebook to give you partial credit.\n",
    "\n",
    "\n",
    "\n",
    " Please provide clear and short answers between `<div class=\"alert alert-info\">  <your answer>  </div>` tags (when it's not code).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "NlwqZF0IUj4b"
   },
   "source": [
    "<div class=\"alert alert-info\">  Example of answer  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_ktag": "NlwqZF0IUj4b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.mkdir(\"data\")\n",
    "if not os.path.exists(\"data/TP4_images\"):\n",
    "    os.mkdir(\"data/TP4_images\")\n",
    "    !cd data/TP4_images && wget -O north_dataset_train.zip  \"https://nextcloud.lisn.upsaclay.fr/index.php/s/yzQRWE2YjmFn9WA/download/north_dataset_train.zip\" && unzip north_dataset_train.zip\n",
    "    !cd data/TP4_images && wget -O north_dataset_test.zip  \"https://nextcloud.lisn.upsaclay.fr/index.php/s/zntidWrFdYsGMDm/download/north_dataset_test.zip\" && unzip north_dataset_test.zip\n",
    "dir_path = \"../data/TP4_images/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "cell_ktag": "NlwqZF0IUj4b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchmetrics.classification import BinaryAccuracy, Accuracy, ConfusionMatrix\n",
    "# you can install torchmetrics with `pip install torchmetrics`\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "north_dataset = datasets.ImageFolder(\n",
    "    dir_path + \"north_dataset_sample\",\n",
    "    transform=transforms.Compose([transforms.ToTensor()]),\n",
    ")\n",
    "test_dataset = datasets.ImageFolder(\n",
    "    dir_path + \"north_dataset_test\",\n",
    "    transform=transforms.Compose([transforms.ToTensor()]),\n",
    ")\n",
    "base_model = models.resnet18()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "NlwqZF0IUj4b"
   },
   "source": [
    "## Question 1 : \n",
    ">  Change the last layer of the resnet model so that its size fits the problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "cell_ktag": "NlwqZF0IUj4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Hint \n",
    "print(base_model)\n",
    "base_model.fc = nn.Linear(base_model.fc.in_features, 2, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "cell_ktag": "NlwqZF0IUj4b"
   },
   "outputs": [],
   "source": [
    "assert (\n",
    "    base_model.fc.out_features == 2\n",
    ")  # we could also change the last layer to have 1 output. Do it with 2 so that it matches our tests procedure during grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class OneHotDataset(Dataset):\n",
    "    def __init__(self, dataset, num_classes=2):\n",
    "        self.dataset = dataset\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, target = self.dataset[idx]\n",
    "        target = torch.nn.functional.one_hot(torch.tensor(target), num_classes=self.num_classes).to(torch.float32)\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "NlwqZF0IUj4b"
   },
   "source": [
    "## Question 2: \n",
    "> Train the last layer of a randomly initialized resnet model. Provide a function precompute_features in `utils.py` that creates a new dataset from the features precomputed by the model.\n",
    "\n",
    "Intermediate question :  Provide the training process in the notebook with training curve. Comment on the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "cell_ktag": "NlwqZF0IUj4b"
   },
   "outputs": [],
   "source": [
    "def precompute_features(\n",
    "    model: models.ResNet, \n",
    "    dataset: torch.utils.data.Dataset, \n",
    "    device: torch.device\n",
    ") -> torch.utils.data.Dataset:\n",
    "    \"\"\"\n",
    "    Create a new dataset with the features precomputed by the model.\n",
    "\n",
    "    If the model is $f \\circ g$ where $f$ is the last layer and $g$ is \n",
    "    the rest of the model, it is not necessary to recompute $g(x)$ at \n",
    "    each epoch as $g$ is fixed. Hence you can precompute $g(x)$ and \n",
    "    create a new dataset \n",
    "    $\\mathcal{X}_{\\text{train}}' = \\{(g(x_n),y_n)\\}_{n\\leq N_{\\text{train}}}$\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "    model: models.ResNet\n",
    "        The model used to precompute the features\n",
    "    dataset: torch.utils.data.Dataset\n",
    "        The dataset to precompute the features from\n",
    "    device: torch.device\n",
    "        The device to use for the computation\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    torch.utils.data.Dataset\n",
    "        The new dataset with the features precomputed\n",
    "    \"\"\"\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    # Remove the last layer (fc) to get the feature extractor part of the model\n",
    "    precompute_model = nn.Sequential(*list(model.children())[:-1])\n",
    "    precompute_model.eval()\n",
    "    precompute_model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            features = precompute_model(X)\n",
    "            features = features.view(features.size(0), -1)  # Flatten the features\n",
    "            features_list.append(features.cpu())\n",
    "            labels_list.append(y.cpu())\n",
    "\n",
    "    features_tensor = torch.cat(features_list)\n",
    "    labels_tensor = torch.cat(labels_list)\n",
    "\n",
    "    new_dataset = torch.utils.data.TensorDataset(features_tensor, labels_tensor)\n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(precompute_features(base_model, OneHotDataset(north_dataset), device), batch_size=64, shuffle=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(precompute_features(base_model, OneHotDataset(test_dataset), device), batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, lr, criterion, dataloader, epochs=500):\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "  losses = []\n",
    "  for _ in range(epochs):\n",
    "    train_loss = 0\n",
    "    for X, y in dataloader:\n",
    "      optimizer.zero_grad()\n",
    "      X = X.to(device)\n",
    "      y = y.to(device)\n",
    "      y_pred = model(X)\n",
    "      loss = criterion(y_pred, y)\n",
    "      loss.backward()\n",
    "      train_loss += loss.item()\n",
    "      optimizer.step()\n",
    "    if _ % 10 == 0:\n",
    "      print(f'Training loss for {_}/{epochs}: {train_loss}')\n",
    "    losses.append(train_loss)\n",
    "  return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for 0/2000: 1.2053449153900146\n",
      "Training loss for 10/2000: 0.9275075793266296\n",
      "Training loss for 20/2000: 0.8838087916374207\n",
      "Training loss for 30/2000: 0.885044276714325\n",
      "Training loss for 40/2000: 0.8815682530403137\n",
      "Training loss for 50/2000: 0.8772154450416565\n",
      "Training loss for 60/2000: 0.8741158246994019\n",
      "Training loss for 70/2000: 0.8714056611061096\n",
      "Training loss for 80/2000: 0.8686467409133911\n",
      "Training loss for 90/2000: 0.8657656311988831\n",
      "Training loss for 100/2000: 0.8627730011940002\n",
      "Training loss for 110/2000: 0.8596793413162231\n",
      "Training loss for 120/2000: 0.856495201587677\n",
      "Training loss for 130/2000: 0.8532296419143677\n",
      "Training loss for 140/2000: 0.8498906493186951\n",
      "Training loss for 150/2000: 0.8464856743812561\n",
      "Training loss for 160/2000: 0.8430210947990417\n",
      "Training loss for 170/2000: 0.8395030498504639\n",
      "Training loss for 180/2000: 0.8359366655349731\n",
      "Training loss for 190/2000: 0.8323270082473755\n",
      "Training loss for 200/2000: 0.8286782503128052\n",
      "Training loss for 210/2000: 0.8249947428703308\n",
      "Training loss for 220/2000: 0.8212803602218628\n",
      "Training loss for 230/2000: 0.8175383806228638\n",
      "Training loss for 240/2000: 0.8137720227241516\n",
      "Training loss for 250/2000: 0.8099845051765442\n",
      "Training loss for 260/2000: 0.8061783313751221\n",
      "Training loss for 270/2000: 0.8023561835289001\n",
      "Training loss for 280/2000: 0.7985203862190247\n",
      "Training loss for 290/2000: 0.7946732640266418\n",
      "Training loss for 300/2000: 0.7908169627189636\n",
      "Training loss for 310/2000: 0.786953330039978\n",
      "Training loss for 320/2000: 0.7830840349197388\n",
      "Training loss for 330/2000: 0.779211163520813\n",
      "Training loss for 340/2000: 0.775335967540741\n",
      "Training loss for 350/2000: 0.7714599370956421\n",
      "Training loss for 360/2000: 0.7675845623016357\n",
      "Training loss for 370/2000: 0.7637112736701965\n",
      "Training loss for 380/2000: 0.7598410844802856\n",
      "Training loss for 390/2000: 0.7559752464294434\n",
      "Training loss for 400/2000: 0.7521148324012756\n",
      "Training loss for 410/2000: 0.7482607960700989\n",
      "Training loss for 420/2000: 0.7444140315055847\n",
      "Training loss for 430/2000: 0.7405755519866943\n",
      "Training loss for 440/2000: 0.7367460131645203\n",
      "Training loss for 450/2000: 0.7329262495040894\n",
      "Training loss for 460/2000: 0.7291171550750732\n",
      "Training loss for 470/2000: 0.7253192067146301\n",
      "Training loss for 480/2000: 0.7215331196784973\n",
      "Training loss for 490/2000: 0.7177594304084778\n",
      "Training loss for 500/2000: 0.7139986753463745\n",
      "Training loss for 510/2000: 0.7102514505386353\n",
      "Training loss for 520/2000: 0.7065181732177734\n",
      "Training loss for 530/2000: 0.7027993202209473\n",
      "Training loss for 540/2000: 0.6990953087806702\n",
      "Training loss for 550/2000: 0.695406436920166\n",
      "Training loss for 560/2000: 0.6917332410812378\n",
      "Training loss for 570/2000: 0.6880759000778198\n",
      "Training loss for 580/2000: 0.684434711933136\n",
      "Training loss for 590/2000: 0.6808100342750549\n",
      "Training loss for 600/2000: 0.6772021055221558\n",
      "Training loss for 610/2000: 0.6736111044883728\n",
      "Training loss for 620/2000: 0.6700373888015747\n",
      "Training loss for 630/2000: 0.6664811372756958\n",
      "Training loss for 640/2000: 0.6629423499107361\n",
      "Training loss for 650/2000: 0.659421443939209\n",
      "Training loss for 660/2000: 0.6559184193611145\n",
      "Training loss for 670/2000: 0.6524333953857422\n",
      "Training loss for 680/2000: 0.6489665508270264\n",
      "Training loss for 690/2000: 0.6455179452896118\n",
      "Training loss for 700/2000: 0.6420878171920776\n",
      "Training loss for 710/2000: 0.638676106929779\n",
      "Training loss for 720/2000: 0.6352829933166504\n",
      "Training loss for 730/2000: 0.6319083571434021\n",
      "Training loss for 740/2000: 0.6285523772239685\n",
      "Training loss for 750/2000: 0.6252151727676392\n",
      "Training loss for 760/2000: 0.6218966245651245\n",
      "Training loss for 770/2000: 0.6185968518257141\n",
      "Training loss for 780/2000: 0.615315854549408\n",
      "Training loss for 790/2000: 0.612053632736206\n",
      "Training loss for 800/2000: 0.6088102459907532\n",
      "Training loss for 810/2000: 0.6055855751037598\n",
      "Training loss for 820/2000: 0.6023796796798706\n",
      "Training loss for 830/2000: 0.5991925597190857\n",
      "Training loss for 840/2000: 0.5960241556167603\n",
      "Training loss for 850/2000: 0.5928744077682495\n",
      "Training loss for 860/2000: 0.5897433757781982\n",
      "Training loss for 870/2000: 0.5866309404373169\n",
      "Training loss for 880/2000: 0.5835371017456055\n",
      "Training loss for 890/2000: 0.5804618000984192\n",
      "Training loss for 900/2000: 0.5774050354957581\n",
      "Training loss for 910/2000: 0.5743667483329773\n",
      "Training loss for 920/2000: 0.5713467001914978\n",
      "Training loss for 930/2000: 0.5683449506759644\n",
      "Training loss for 940/2000: 0.565361499786377\n",
      "Training loss for 950/2000: 0.5623963475227356\n",
      "Training loss for 960/2000: 0.5594490766525269\n",
      "Training loss for 970/2000: 0.5565199851989746\n",
      "Training loss for 980/2000: 0.553608775138855\n",
      "Training loss for 990/2000: 0.550715446472168\n",
      "Training loss for 1000/2000: 0.5478399991989136\n",
      "Training loss for 1010/2000: 0.5449822545051575\n",
      "Training loss for 1020/2000: 0.5421420931816101\n",
      "Training loss for 1030/2000: 0.5393194556236267\n",
      "Training loss for 1040/2000: 0.5365142822265625\n",
      "Training loss for 1050/2000: 0.5337264537811279\n",
      "Training loss for 1060/2000: 0.5309559106826782\n",
      "Training loss for 1070/2000: 0.5282025337219238\n",
      "Training loss for 1080/2000: 0.52546626329422\n",
      "Training loss for 1090/2000: 0.5227469205856323\n",
      "Training loss for 1100/2000: 0.5200444459915161\n",
      "Training loss for 1110/2000: 0.5173588991165161\n",
      "Training loss for 1120/2000: 0.5146899223327637\n",
      "Training loss for 1130/2000: 0.5120375156402588\n",
      "Training loss for 1140/2000: 0.5094016790390015\n",
      "Training loss for 1150/2000: 0.5067821741104126\n",
      "Training loss for 1160/2000: 0.5041789412498474\n",
      "Training loss for 1170/2000: 0.5015919208526611\n",
      "Training loss for 1180/2000: 0.49902108311653137\n",
      "Training loss for 1190/2000: 0.49646610021591187\n",
      "Training loss for 1200/2000: 0.4939269423484802\n",
      "Training loss for 1210/2000: 0.4914036691188812\n",
      "Training loss for 1220/2000: 0.4888960123062134\n",
      "Training loss for 1230/2000: 0.4864039123058319\n",
      "Training loss for 1240/2000: 0.48392727971076965\n",
      "Training loss for 1250/2000: 0.4814659655094147\n",
      "Training loss for 1260/2000: 0.4790199398994446\n",
      "Training loss for 1270/2000: 0.4765889644622803\n",
      "Training loss for 1280/2000: 0.4741731286048889\n",
      "Training loss for 1290/2000: 0.47177234292030334\n",
      "Training loss for 1300/2000: 0.4693860411643982\n",
      "Training loss for 1310/2000: 0.4670146703720093\n",
      "Training loss for 1320/2000: 0.4646579325199127\n",
      "Training loss for 1330/2000: 0.46231573820114136\n",
      "Training loss for 1340/2000: 0.4599878191947937\n",
      "Training loss for 1350/2000: 0.4576743543148041\n",
      "Training loss for 1360/2000: 0.45537495613098145\n",
      "Training loss for 1370/2000: 0.45308977365493774\n",
      "Training loss for 1380/2000: 0.45081856846809387\n",
      "Training loss for 1390/2000: 0.44856125116348267\n",
      "Training loss for 1400/2000: 0.4463176727294922\n",
      "Training loss for 1410/2000: 0.44408783316612244\n",
      "Training loss for 1420/2000: 0.44187164306640625\n",
      "Training loss for 1430/2000: 0.4396688938140869\n",
      "Training loss for 1440/2000: 0.4374794661998749\n",
      "Training loss for 1450/2000: 0.4353034198284149\n",
      "Training loss for 1460/2000: 0.4331405758857727\n",
      "Training loss for 1470/2000: 0.43099066615104675\n",
      "Training loss for 1480/2000: 0.42885392904281616\n",
      "Training loss for 1490/2000: 0.42673006653785706\n",
      "Training loss for 1500/2000: 0.42461898922920227\n",
      "Training loss for 1510/2000: 0.4225206971168518\n",
      "Training loss for 1520/2000: 0.4204348921775818\n",
      "Training loss for 1530/2000: 0.4183616042137146\n",
      "Training loss for 1540/2000: 0.4163009226322174\n",
      "Training loss for 1550/2000: 0.4142523407936096\n",
      "Training loss for 1560/2000: 0.41221609711647034\n",
      "Training loss for 1570/2000: 0.4101920425891876\n",
      "Training loss for 1580/2000: 0.40817996859550476\n",
      "Training loss for 1590/2000: 0.4061799645423889\n",
      "Training loss for 1600/2000: 0.40419164299964905\n",
      "Training loss for 1610/2000: 0.4022153317928314\n",
      "Training loss for 1620/2000: 0.40025049448013306\n",
      "Training loss for 1630/2000: 0.39829736948013306\n",
      "Training loss for 1640/2000: 0.3963557183742523\n",
      "Training loss for 1650/2000: 0.39442554116249084\n",
      "Training loss for 1660/2000: 0.3925066590309143\n",
      "Training loss for 1670/2000: 0.3905991017818451\n",
      "Training loss for 1680/2000: 0.3887026607990265\n",
      "Training loss for 1690/2000: 0.38681721687316895\n",
      "Training loss for 1700/2000: 0.3849429488182068\n",
      "Training loss for 1710/2000: 0.38307952880859375\n",
      "Training loss for 1720/2000: 0.38122689723968506\n",
      "Training loss for 1730/2000: 0.3793851137161255\n",
      "Training loss for 1740/2000: 0.37755388021469116\n",
      "Training loss for 1750/2000: 0.37573328614234924\n",
      "Training loss for 1760/2000: 0.3739232122898102\n",
      "Training loss for 1770/2000: 0.3721235394477844\n",
      "Training loss for 1780/2000: 0.370334267616272\n",
      "Training loss for 1790/2000: 0.3685552477836609\n",
      "Training loss for 1800/2000: 0.3667864501476288\n",
      "Training loss for 1810/2000: 0.3650277256965637\n",
      "Training loss for 1820/2000: 0.36327898502349854\n",
      "Training loss for 1830/2000: 0.3615403473377228\n",
      "Training loss for 1840/2000: 0.35981154441833496\n",
      "Training loss for 1850/2000: 0.3580925464630127\n",
      "Training loss for 1860/2000: 0.3563833236694336\n",
      "Training loss for 1870/2000: 0.35468363761901855\n",
      "Training loss for 1880/2000: 0.35299378633499146\n",
      "Training loss for 1890/2000: 0.3513133227825165\n",
      "Training loss for 1900/2000: 0.34964239597320557\n",
      "Training loss for 1910/2000: 0.3479807376861572\n",
      "Training loss for 1920/2000: 0.34632840752601624\n",
      "Training loss for 1930/2000: 0.34468531608581543\n",
      "Training loss for 1940/2000: 0.3430514931678772\n",
      "Training loss for 1950/2000: 0.3414267599582672\n",
      "Training loss for 1960/2000: 0.3398109972476959\n",
      "Training loss for 1970/2000: 0.3382043242454529\n",
      "Training loss for 1980/2000: 0.3366064429283142\n",
      "Training loss for 1990/2000: 0.33501744270324707\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "base_model.fc = nn.Linear(512, 2)\n",
    "linear_classifier = nn.Linear(base_model.fc.in_features, 2, bias=True).to(device)\n",
    "losses = train_model(linear_classifier, 1e-3, nn.BCEWithLogitsLoss(weight=torch.Tensor([1., 2.])), train_dataloader, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def eval_model(model, criterion, test_dataloader):\n",
    "  model.eval()\n",
    "  test_error = 0\n",
    "  all_preds = []\n",
    "  all_labels = []\n",
    "  for X, y in test_dataloader:\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "    y_pred = model(X)\n",
    "    test_error += criterion(y_pred, y).item()\n",
    "    all_preds.append(y_pred.argmax(dim=1).cpu())\n",
    "    all_labels.append(y.argmax(dim=1).cpu())\n",
    "  test_error /= len(test_dataloader)\n",
    "  print(f'Test error: {test_error}')\n",
    "  \n",
    "  all_preds = torch.cat(all_preds).flatten()\n",
    "  all_labels = torch.cat(all_labels).flatten()\n",
    "  cm = ConfusionMatrix(task='binary', num_classes=2)\n",
    "  confusion_matrix = cm(all_preds, all_labels)\n",
    "  print(f'Confusion Matrix:\\n{confusion_matrix}')\n",
    "  \n",
    "  acc = Accuracy(task=\"binary\", num_classes=2)\n",
    "  print(f'Accuracy: {acc(all_preds, all_labels)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error: 0.7794966101646423\n",
      "Confusion Matrix:\n",
      "tensor([[21,  0],\n",
      "        [21,  0]])\n",
      "Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "eval_model(base_model.fc, nn.BCEWithLogitsLoss(), test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x21fa77c8640>]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "figure = plt.figure()\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "NlwqZF0IUj4b"
   },
   "outputs": [],
   "source": [
    "from utils import precompute_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "NlwqZF0IUj4b"
   },
   "source": [
    "<div class=\"alert alert-info\">  Example of answer  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "NlwqZF0IUj4b"
   },
   "source": [
    "## Question 3 : \n",
    "> Now start from a pretained model on Imagenet (https://pytorch.org/vision/stable/models.html#) and only train the last layer. Provide the training process in the notebook with training curve. \n",
    "\n",
    " Provide two files : (https://pytorch.org/tutorials/beginner/saving_loading_models.html)\n",
    " -  a file  `utils.py` containing only the last layer class `LastLayer` inheriting from `torch.nn.Module` architecture of your final model to load\n",
    " -  a `last_layer_finetune.pth` file containing __only the last layer weights__ ( we will check the size) \n",
    " \n",
    " We will test your model on final accuracy on a test set. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LastLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LastLayer, self).__init__()\n",
    "        self.fc1 = nn.Linear(512, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): LastLayer(\n",
       "    (fc1): Linear(in_features=512, out_features=2, bias=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from utils import LastLayer\n",
    "\n",
    "resnet = models.resnet18(weights=\"DEFAULT\")\n",
    "fc = LastLayer()  # !  Important : No argument\n",
    "#fc.load_state_dict(torch.load(\"lastlayer.pth\", weights_only=True))\n",
    "resnet.fc = fc\n",
    "resnet.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for 0/2000: 13.820637702941895\n",
      "Training loss for 10/2000: 12.37720012664795\n",
      "Training loss for 20/2000: 10.93376350402832\n",
      "Training loss for 30/2000: 9.490324020385742\n",
      "Training loss for 40/2000: 8.046890258789062\n",
      "Training loss for 50/2000: 6.6037139892578125\n",
      "Training loss for 60/2000: 5.174555778503418\n",
      "Training loss for 70/2000: 3.8901143074035645\n",
      "Training loss for 80/2000: 2.8274216651916504\n",
      "Training loss for 90/2000: 1.9750540256500244\n",
      "Training loss for 100/2000: 1.3978400230407715\n",
      "Training loss for 110/2000: 1.1206529140472412\n",
      "Training loss for 120/2000: 1.0351314544677734\n",
      "Training loss for 130/2000: 0.9931834936141968\n",
      "Training loss for 140/2000: 0.9487006664276123\n",
      "Training loss for 150/2000: 0.9027615785598755\n",
      "Training loss for 160/2000: 0.8592334985733032\n",
      "Training loss for 170/2000: 0.8180869817733765\n",
      "Training loss for 180/2000: 0.7784451246261597\n",
      "Training loss for 190/2000: 0.7400522232055664\n",
      "Training loss for 200/2000: 0.7029698491096497\n",
      "Training loss for 210/2000: 0.6672466993331909\n",
      "Training loss for 220/2000: 0.6329006552696228\n",
      "Training loss for 230/2000: 0.5999583601951599\n",
      "Training loss for 240/2000: 0.5684716105461121\n",
      "Training loss for 250/2000: 0.5385221838951111\n",
      "Training loss for 260/2000: 0.5102195739746094\n",
      "Training loss for 270/2000: 0.48369067907333374\n",
      "Training loss for 280/2000: 0.4590510427951813\n",
      "Training loss for 290/2000: 0.43637076020240784\n",
      "Training loss for 300/2000: 0.4156482219696045\n",
      "Training loss for 310/2000: 0.3968040943145752\n",
      "Training loss for 320/2000: 0.3796958923339844\n",
      "Training loss for 330/2000: 0.36414143443107605\n",
      "Training loss for 340/2000: 0.34994426369667053\n",
      "Training loss for 350/2000: 0.33691421151161194\n",
      "Training loss for 360/2000: 0.32487696409225464\n",
      "Training loss for 370/2000: 0.31367987394332886\n",
      "Training loss for 380/2000: 0.3031933307647705\n",
      "Training loss for 390/2000: 0.2933090627193451\n",
      "Training loss for 400/2000: 0.283937007188797\n",
      "Training loss for 410/2000: 0.2750020921230316\n",
      "Training loss for 420/2000: 0.26644381880760193\n",
      "Training loss for 430/2000: 0.2582113742828369\n",
      "Training loss for 440/2000: 0.25026386976242065\n",
      "Training loss for 450/2000: 0.24256740510463715\n",
      "Training loss for 460/2000: 0.23509393632411957\n",
      "Training loss for 470/2000: 0.2278217375278473\n",
      "Training loss for 480/2000: 0.22073207795619965\n",
      "Training loss for 490/2000: 0.21381092071533203\n",
      "Training loss for 500/2000: 0.2070465385913849\n",
      "Training loss for 510/2000: 0.20043013989925385\n",
      "Training loss for 520/2000: 0.19395564496517181\n",
      "Training loss for 530/2000: 0.1876177042722702\n",
      "Training loss for 540/2000: 0.1814141571521759\n",
      "Training loss for 550/2000: 0.17534294724464417\n",
      "Training loss for 560/2000: 0.16940371692180634\n",
      "Training loss for 570/2000: 0.16359731554985046\n",
      "Training loss for 580/2000: 0.15792496502399445\n",
      "Training loss for 590/2000: 0.15238884091377258\n",
      "Training loss for 600/2000: 0.14699120819568634\n",
      "Training loss for 610/2000: 0.14173467457294464\n",
      "Training loss for 620/2000: 0.13662222027778625\n",
      "Training loss for 630/2000: 0.131656676530838\n",
      "Training loss for 640/2000: 0.1268402487039566\n",
      "Training loss for 650/2000: 0.12217552214860916\n",
      "Training loss for 660/2000: 0.11766413599252701\n",
      "Training loss for 670/2000: 0.11330743134021759\n",
      "Training loss for 680/2000: 0.10910597443580627\n",
      "Training loss for 690/2000: 0.10505980253219604\n",
      "Training loss for 700/2000: 0.10116872191429138\n",
      "Training loss for 710/2000: 0.0974312275648117\n",
      "Training loss for 720/2000: 0.09384554624557495\n",
      "Training loss for 730/2000: 0.0904093012213707\n",
      "Training loss for 740/2000: 0.08711950480937958\n",
      "Training loss for 750/2000: 0.08397265523672104\n",
      "Training loss for 760/2000: 0.08096498996019363\n",
      "Training loss for 770/2000: 0.07809215784072876\n",
      "Training loss for 780/2000: 0.07534971088171005\n",
      "Training loss for 790/2000: 0.0727328509092331\n",
      "Training loss for 800/2000: 0.0702369213104248\n",
      "Training loss for 810/2000: 0.06785677373409271\n",
      "Training loss for 820/2000: 0.06558734178543091\n",
      "Training loss for 830/2000: 0.06342379003763199\n",
      "Training loss for 840/2000: 0.0613609254360199\n",
      "Training loss for 850/2000: 0.059394221752882004\n",
      "Training loss for 860/2000: 0.05751881003379822\n",
      "Training loss for 870/2000: 0.05572987720370293\n",
      "Training loss for 880/2000: 0.05402325838804245\n",
      "Training loss for 890/2000: 0.05239446461200714\n",
      "Training loss for 900/2000: 0.050839588046073914\n",
      "Training loss for 910/2000: 0.04935453459620476\n",
      "Training loss for 920/2000: 0.047935549169778824\n",
      "Training loss for 930/2000: 0.04657912626862526\n",
      "Training loss for 940/2000: 0.0452820248901844\n",
      "Training loss for 950/2000: 0.04404086619615555\n",
      "Training loss for 960/2000: 0.04285280406475067\n",
      "Training loss for 970/2000: 0.04171481728553772\n",
      "Training loss for 980/2000: 0.04062429815530777\n",
      "Training loss for 990/2000: 0.03957877680659294\n",
      "Training loss for 1000/2000: 0.038575779646635056\n",
      "Training loss for 1010/2000: 0.03761300817131996\n",
      "Training loss for 1020/2000: 0.0366884283721447\n",
      "Training loss for 1030/2000: 0.03580009937286377\n",
      "Training loss for 1040/2000: 0.03494603559374809\n",
      "Training loss for 1050/2000: 0.03412456437945366\n",
      "Training loss for 1060/2000: 0.03333396092057228\n",
      "Training loss for 1070/2000: 0.03257272392511368\n",
      "Training loss for 1080/2000: 0.031839389353990555\n",
      "Training loss for 1090/2000: 0.03113262727856636\n",
      "Training loss for 1100/2000: 0.030450968071818352\n",
      "Training loss for 1110/2000: 0.02979331649839878\n",
      "Training loss for 1120/2000: 0.029158499091863632\n",
      "Training loss for 1130/2000: 0.028545478358864784\n",
      "Training loss for 1140/2000: 0.02795320376753807\n",
      "Training loss for 1150/2000: 0.027380673214793205\n",
      "Training loss for 1160/2000: 0.026827005669474602\n",
      "Training loss for 1170/2000: 0.026291290298104286\n",
      "Training loss for 1180/2000: 0.025772875174880028\n",
      "Training loss for 1190/2000: 0.02527076005935669\n",
      "Training loss for 1200/2000: 0.02478448860347271\n",
      "Training loss for 1210/2000: 0.02431328594684601\n",
      "Training loss for 1220/2000: 0.023856384679675102\n",
      "Training loss for 1230/2000: 0.023413266986608505\n",
      "Training loss for 1240/2000: 0.022983362898230553\n",
      "Training loss for 1250/2000: 0.022566121071577072\n",
      "Training loss for 1260/2000: 0.022161010652780533\n",
      "Training loss for 1270/2000: 0.021767528727650642\n",
      "Training loss for 1280/2000: 0.021385177969932556\n",
      "Training loss for 1290/2000: 0.021013597026467323\n",
      "Training loss for 1300/2000: 0.020652329549193382\n",
      "Training loss for 1310/2000: 0.02030094712972641\n",
      "Training loss for 1320/2000: 0.0199591014534235\n",
      "Training loss for 1330/2000: 0.019626434892416\n",
      "Training loss for 1340/2000: 0.019302550703287125\n",
      "Training loss for 1350/2000: 0.018987182527780533\n",
      "Training loss for 1360/2000: 0.018679950386285782\n",
      "Training loss for 1370/2000: 0.01838066428899765\n",
      "Training loss for 1380/2000: 0.01808890700340271\n",
      "Training loss for 1390/2000: 0.017804495990276337\n",
      "Training loss for 1400/2000: 0.017527177929878235\n",
      "Training loss for 1410/2000: 0.017256688326597214\n",
      "Training loss for 1420/2000: 0.016992757096886635\n",
      "Training loss for 1430/2000: 0.016735177487134933\n",
      "Training loss for 1440/2000: 0.01648375578224659\n",
      "Training loss for 1450/2000: 0.01623820699751377\n",
      "Training loss for 1460/2000: 0.015998438000679016\n",
      "Training loss for 1470/2000: 0.015764249488711357\n",
      "Training loss for 1480/2000: 0.015535444021224976\n",
      "Training loss for 1490/2000: 0.015311772003769875\n",
      "Training loss for 1500/2000: 0.01509317010641098\n",
      "Training loss for 1510/2000: 0.014879418537020683\n",
      "Training loss for 1520/2000: 0.014670406468212605\n",
      "Training loss for 1530/2000: 0.014465908519923687\n",
      "Training loss for 1540/2000: 0.014265835285186768\n",
      "Training loss for 1550/2000: 0.014070103876292706\n",
      "Training loss for 1560/2000: 0.0138785969465971\n",
      "Training loss for 1570/2000: 0.013690982945263386\n",
      "Training loss for 1580/2000: 0.01350735779851675\n",
      "Training loss for 1590/2000: 0.01332755759358406\n",
      "Training loss for 1600/2000: 0.013151455670595169\n",
      "Training loss for 1610/2000: 0.012978906743228436\n",
      "Training loss for 1620/2000: 0.012809856794774532\n",
      "Training loss for 1630/2000: 0.01264413632452488\n",
      "Training loss for 1640/2000: 0.01248178631067276\n",
      "Training loss for 1650/2000: 0.012322582304477692\n",
      "Training loss for 1660/2000: 0.012166528031229973\n",
      "Training loss for 1670/2000: 0.012013467028737068\n",
      "Training loss for 1680/2000: 0.011863362044095993\n",
      "Training loss for 1690/2000: 0.011716105975210667\n",
      "Training loss for 1700/2000: 0.011571638286113739\n",
      "Training loss for 1710/2000: 0.011429860256612301\n",
      "Training loss for 1720/2000: 0.011290833353996277\n",
      "Training loss for 1730/2000: 0.01115422323346138\n",
      "Training loss for 1740/2000: 0.011020146310329437\n",
      "Training loss for 1750/2000: 0.010888554155826569\n",
      "Training loss for 1760/2000: 0.010759337805211544\n",
      "Training loss for 1770/2000: 0.010632400400936604\n",
      "Training loss for 1780/2000: 0.010507759638130665\n",
      "Training loss for 1790/2000: 0.010385347530245781\n",
      "Training loss for 1800/2000: 0.010265020653605461\n",
      "Training loss for 1810/2000: 0.010146798565983772\n",
      "Training loss for 1820/2000: 0.010030634701251984\n",
      "Training loss for 1830/2000: 0.009916447103023529\n",
      "Training loss for 1840/2000: 0.009804175235331059\n",
      "Training loss for 1850/2000: 0.00969389732927084\n",
      "Training loss for 1860/2000: 0.009585361927747726\n",
      "Training loss for 1870/2000: 0.009478743188083172\n",
      "Training loss for 1880/2000: 0.00937379989773035\n",
      "Training loss for 1890/2000: 0.009270678274333477\n",
      "Training loss for 1900/2000: 0.009169151075184345\n",
      "Training loss for 1910/2000: 0.009069330990314484\n",
      "Training loss for 1920/2000: 0.008971059694886208\n",
      "Training loss for 1930/2000: 0.008874429389834404\n",
      "Training loss for 1940/2000: 0.008779359981417656\n",
      "Training loss for 1950/2000: 0.008685771375894547\n",
      "Training loss for 1960/2000: 0.008593600243330002\n",
      "Training loss for 1970/2000: 0.008502880111336708\n",
      "Training loss for 1980/2000: 0.00841359794139862\n",
      "Training loss for 1990/2000: 0.008325662463903427\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(precompute_features(resnet, OneHotDataset(north_dataset), device), batch_size=64, shuffle=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(precompute_features(resnet, OneHotDataset(test_dataset), device), batch_size=64, shuffle=False)\n",
    "losses = train_model(resnet.fc, 1e-3, nn.BCEWithLogitsLoss(weight=torch.Tensor([1., 2.])), train_dataloader, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error: 1.7527709007263184\n",
      "Confusion Matrix:\n",
      "tensor([[19,  2],\n",
      "        [ 7, 14]])\n",
      "Accuracy: 0.7857142686843872\n"
     ]
    }
   ],
   "source": [
    "eval_model(resnet.fc.to(device), nn.BCEWithLogitsLoss(), test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "cell_ktag": "NlwqZF0IUj4b"
   },
   "outputs": [],
   "source": [
    "torch.save(base_model.fc.state_dict(), \"lastlayer.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "NlwqZF0IUj4b"
   },
   "source": [
    "## Question 4 : \n",
    "> Perform  LoRA https://arxiv.org/pdf/2106.09685  on the model  (We are perfectly fine if you use an external library **for this question only**, and of course use it in the next questions). (Warning : without data augmentation it may not improve the accuracy.)\n",
    "\n",
    "Intermediate question : Describe LoRA. There are different ways of implementing LoRa for convolutions. You can choose your preferred one. Explain the version of LoRa you used, provide a drawing of the process in the `drawing_lora.png` file. (Hint: you can obtain a small rank convolution by combining a convolution and a 1x1 convolution. One of the two goes from a higher number of channels to a lower number of channels and the other one restores the number of channels.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRAConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, rank, stride=1, padding=0, bias=False):\n",
    "        super(LoRAConv, self).__init__()\n",
    "        \n",
    "        # First convolution reduces the number of channels (compression)\n",
    "        self.conv1 = nn.Conv2d(in_channels, rank, kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "        \n",
    "        # Second 1x1 convolution restores the number of channels\n",
    "        self.conv2 = nn.Conv2d(rank, out_channels, kernel_size=1, stride=1, padding=0, bias=bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "    \n",
    "class LoRAResNet(nn.Module):\n",
    "  def __init__(self, base_model, rank=4):\n",
    "    super(LoRAResNet, self).__init__()\n",
    "    self.base_model = base_model\n",
    "\n",
    "    for parameter in self.base_model.parameters():\n",
    "        parameter.requires_grad = False\n",
    "\n",
    "    # Replace convolution layers with LoRA convolution layers\n",
    "    temp = list(self.base_model.named_modules())\n",
    "    for name, module in temp:\n",
    "        if isinstance(module, nn.Conv2d) and module.kernel_size != (1, 1):  # Avoid modifying 1x1 convs\n",
    "            in_channels = module.in_channels\n",
    "            out_channels = module.out_channels\n",
    "            kernel_size = module.kernel_size\n",
    "            stride = module.stride\n",
    "            padding = module.padding\n",
    "        \n",
    "            # Replace with LoRA convolution\n",
    "            lora_conv = LoRAConv(in_channels, out_channels, kernel_size, rank, stride, padding)\n",
    "            setattr(self.base_model, name, lora_conv)\n",
    "    print(self.base_model)          \n",
    "      \n",
    "  \n",
    "  def forward(self, x):\n",
    "      return self.base_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lora_pytorch import LoRA\n",
    "base_resnet = models.resnet18(weights='DEFAULT')\n",
    "base_resnet.fc = nn.Linear(base_resnet.fc.in_features, 2, bias=True)\n",
    "lora_resnet = LoRA.from_module(base_resnet, rank=5)\n",
    "#lora_resnet = LoRAResNet(base_resnet, rank=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(OneHotDataset(north_dataset), batch_size=64, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(OneHotDataset(test_dataset), batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for 0/50: 0.7859816551208496\n",
      "Training loss for 10/50: 0.5604530572891235\n",
      "Training loss for 20/50: 0.3740842044353485\n",
      "Training loss for 30/50: 0.38268592953681946\n",
      "Training loss for 40/50: 0.287068635225296\n"
     ]
    }
   ],
   "source": [
    "losses = train_model(lora_resnet.to(device), 0.001, nn.BCEWithLogitsLoss(), train_dataloader, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x21fabcba250>]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error: 0.7936198115348816\n",
      "Confusion Matrix:\n",
      "tensor([[21,  0],\n",
      "        [ 9, 12]])\n",
      "Accuracy: 0.7857142686843872\n"
     ]
    }
   ],
   "source": [
    "eval_model(lora_resnet, nn.BCEWithLogitsLoss(), test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "NlwqZF0IUj4b"
   },
   "outputs": [],
   "source": [
    "torch.save(lora_model.state_dict(), \"lora_resnet.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "NlwqZF0IUj4b"
   },
   "source": [
    "## Question 5 : \n",
    "In order to better train our LORA weights, let's do some Data Augmentation https://en.wikipedia.org/wiki/Data_augmentation . Load some alteration of the data from the `torchvision.transforms` module and incorporate them in your training pipeline.\n",
    "\n",
    " Intermediate question : Check CutMix  (https://pytorch.org/vision/stable/auto_examples/transforms/plot_cutmix_mixup.html#sphx-glr-auto-examples-transforms-plot-cutmix-mixup-py) and explain it with a small drawing `cutmix.png`. \n",
    "\n",
    "\n",
    "  Provide one file : (https://pytorch.org/tutorials/beginner/saving_loading_models.html)\n",
    " -  a `daug_resnet.pth` file containing the weight of the ResNet18 after DAUG  (  !  It  has to be of the class ResNet so you have to merge LoRA weights with the ResNet18 weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "NlwqZF0IUj4b"
   },
   "outputs": [],
   "source": [
    "## Data Augmentation\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "NlwqZF0IUj4b"
   },
   "outputs": [],
   "source": [
    "lora_model = NotImplementedError  # <YOUR CODE>\n",
    "assert isinstance(lora_model, models.ResNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "NlwqZF0IUj4b"
   },
   "outputs": [],
   "source": [
    "torch.save(lora_model.state_dict(), \"daug_resnet.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "NlwqZF0IUj4b"
   },
   "source": [
    "## Question 6 : (BONUS) \n",
    "> Do the best you can : improve performance on test set while keeping ResNet 18 architecture, or decrease the size of the model\n",
    "\n",
    "Provide a file  `final_model.pth` containing the weights of the final model and provide the class `FinalModel()` in the `utils.py` file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "NlwqZF0IUj4b"
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlip-env",
   "language": "python",
   "name": "python3"
  },
  "kfiletag": "NlwqZF0IUj4b",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
